



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="http://learn2learn.net/docs/learn2learn.vision/">
      
      
        <meta name="author" content="Séb Arnold">
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/img/favicons/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.3">
    
    
      
        <title>learn2learn.vision - learn2learn</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.adb8469c.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#2196f3">
      
    
    
      <script src="../../assets/javascripts/modernizr.86422ebf.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>body,input{font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Ubuntu Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
    
      <link rel="stylesheet" href="../../assets/css/l2l_material.css">
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-68693545-3", "seba-1511.github.com")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue" data-md-color-accent="orange">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#learn2learnvisionmodels" tabindex="0" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://learn2learn.net" title="learn2learn" aria-label="learn2learn" class="md-header-nav__button md-logo">
          
            <img alt="logo" src="../../assets/img/learn2learn_white.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              learn2learn
            </span>
            <span class="md-header-nav__topic">
              
                learn2learn.vision
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" aria-label="search" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/learnables/learn2learn/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    learnables/learn2learn
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://learn2learn.net" title="learn2learn" class="md-nav__button md-logo">
      
        <img alt="logo" src="../../assets/img/learn2learn_white.png" width="48" height="48">
      
    </a>
    learn2learn
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/learnables/learn2learn/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    learnables/learn2learn
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Tutorials
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Tutorials
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../tutorials/getting_started/" title="Getting Started" class="md-nav__link">
      Getting Started
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      Documentation
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Documentation
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../learn2learn/" title="learn2learn" class="md-nav__link">
      learn2learn
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../learn2learn.algorithms/" title="learn2learn.algorithms" class="md-nav__link">
      learn2learn.algorithms
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../learn2learn.data/" title="learn2learn.data" class="md-nav__link">
      learn2learn.data
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../learn2learn.gym/" title="learn2learn.gym" class="md-nav__link">
      learn2learn.gym
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        learn2learn.vision
      </label>
    
    <a href="./" title="learn2learn.vision" class="md-nav__link md-nav__link--active">
      learn2learn.vision
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#omniglotfc" class="md-nav__link">
    OmniglotFC
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#omniglotcnn" class="md-nav__link">
    OmniglotCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#miniimagenetcnn" class="md-nav__link">
    MiniImagenetCNN
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Examples
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Examples
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../examples.vision/" title="Computer Vision" class="md-nav__link">
      Computer Vision
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../examples.rl/" title="Reinforcement Learning" class="md-nav__link">
      Reinforcement Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../../changelog/" title="Changelog" class="md-nav__link">
      Changelog
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="https://github.com/learnables/learn2learn/" title="GitHub" class="md-nav__link">
      GitHub
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#omniglotfc" class="md-nav__link">
    OmniglotFC
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#omniglotcnn" class="md-nav__link">
    OmniglotCNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#miniimagenetcnn" class="md-nav__link">
    MiniImagenetCNN
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/learnables/learn2learn/edit/master/docs/docs/learn2learn.vision.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="learn2learnvisionmodels">learn2learn.vision.models<a class="headerlink" href="#learn2learnvisionmodels" title="Permanent link">&para;</a></h1>
<p><strong>Description</strong></p>
<p>A set of commonly used models for meta-learning vision tasks.</p>
<h2 id="omniglotfc">OmniglotFC<a class="headerlink" href="#omniglotfc" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">OmniglotFC</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016.</p>
<p><strong>References</strong></p>
<ol>
<li>Santoro et al. 2016. “Meta-Learning with Memory-Augmented Neural Networks.” ICML.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>input_size</strong> (int) - The dimensionality of the input.</li>
<li><strong>output_size</strong> (int) - The dimensionality of the output.</li>
<li><strong>sizes</strong> (list, <em>optional</em>, default=None) - A list of hidden layer sizes.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">net</span> <span class="o">=</span> <span class="n">OmniglotFC</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">28</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                 <span class="n">sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
</code></pre></div>
</td></tr></table>

<h2 id="omniglotcnn">OmniglotCNN<a class="headerlink" href="#omniglotcnn" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">OmniglotCNN</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="">Source</a></p>
<p><strong>Description</strong></p>
<p>The convolutional network commonly used for Omniglot, as described by Finn et al, 2017.</p>
<p>This network assumes inputs of shapes (1, 28, 28).</p>
<p><strong>References</strong></p>
<ol>
<li>Finn et al. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” ICML.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>output_size</strong> (int) - The dimensionality of the network's output.</li>
<li><strong>hidden_size</strong> (int, <em>optional</em>, default=64) - The dimensionality of the hidden representation.</li>
<li><strong>layers</strong> (int, <em>optional</em>, default=4) - The number of convolutional layers.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">OmniglotCNN</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="miniimagenetcnn">MiniImagenetCNN<a class="headerlink" href="#miniimagenetcnn" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">MiniImagenetCNN</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017.</p>
<p>This network assumes inputs of shapes (3, 84, 84).</p>
<p><strong>References</strong></p>
<ol>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>output_size</strong> (int) - The dimensionality of the network's output.</li>
<li><strong>hidden_size</strong> (int, <em>optional</em>, default=32) - The dimensionality of the hidden representation.</li>
<li><strong>layers</strong> (int, <em>optional</em>, default=4) - The number of convolutional layers.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">MiniImagenetCNN</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h1 id="learn2learnvisiondatasets">learn2learn.vision.datasets<a class="headerlink" href="#learn2learnvisiondatasets" title="Permanent link">&para;</a></h1>
<p><strong>Description</strong></p>
<p>Some datasets commonly used in meta-learning vision tasks.</p>
<h2 id="fullomniglot">FullOmniglot<a class="headerlink" href="#fullomniglot" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">FullOmniglot</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>This class provides an interface to the Omniglot dataset.</p>
<p>The Omniglot dataset was introduced by Lake et al., 2015.
Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples.
While the original dataset is separated in background and evaluation sets,
this class concatenates both sets and leaves to the user the choice of classes splitting
as was done in Ravi and Larochelle, 2017.
The background and evaluation splits are available in the <code>torchvision</code> package.</p>
<p><strong>References</strong></p>
<ol>
<li>Lake et al. 2015. “Human-Level Concept Learning through Probabilistic Program Induction.” Science.</li>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5
6
7
8</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">omniglot</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FullOmniglot</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span>
                                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                                                <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">LANCZOS</span><span class="p">),</span>
                                                <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span>
                                            <span class="p">]),</span>
                                            <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">omniglot</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">omniglot</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="miniimagenet">MiniImagenet<a class="headerlink" href="#miniimagenet" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">MiniImagenet</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/mini_imagenet.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The <em>mini</em>-ImageNet dataset was originally introduced by Vinyals et al., 2016.</p>
<p>It consists of 60'000 colour images of sizes 84x84 pixels.
The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.
The classes are sampled from the ImageNet dataset, and we use the splits from Ravi &amp; Larochelle, 2017.</p>
<p><strong>References</strong></p>
<ol>
<li>Vinyals et al. 2016. “Matching Networks for One Shot Learning.” NeurIPS.</li>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">MiniImagenet</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskGenerator</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">ways</span><span class="o">=</span><span class="n">ways</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="tieredimagenet">TieredImagenet<a class="headerlink" href="#tieredimagenet" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">TieredImagenet</span><span class="p">(</span><span class="n">root</span><span class="p">,</span>
               <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
               <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/tiered_imagenet.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The <em>tiered</em>-ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository.</p>
<p>Like <em>mini</em>-ImageNet, <em>tiered</em>-ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100.
The train-validation-test split is made such that classes from similar categories are in the same splits.
There are 34 categories each containing between 10 and 30 classes.
Of these categories, 20 (351 classes; 448,695 images) are used for training,
6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing.</p>
<p><strong>References</strong></p>
<ol>
<li>Ren et al, 2018. "Meta-Learning for Semi-Supervised Few-Shot Classification." ICLR '18.</li>
<li>Ren Mengye. 2018. "few-shot-ssl-public". <a href="https://github.com/renmengye/few-shot-ssl-public">https://github.com/renmengye/few-shot-ssl-public</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">TieredImagenet</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskDataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="fc100">FC100<a class="headerlink" href="#fc100" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">FC100</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fc100.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The FC100 dataset was originally introduced by Oreshkin et al., 2018.</p>
<p>It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are
split so as to minimize the information overlap between splits.
The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training,
4 (20 classes) for validation, and 4 (20 classes) for testing.
Each class contains 600 images.
The specific splits are provided in the Supplementary Material of the paper.
Our data is downloaded from the link provided by [2].</p>
<p><strong>References</strong></p>
<ol>
<li>Oreshkin et al. 2018. "TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning." NeurIPS.</li>
<li>Kwoonjoon Lee. 2019. "MetaOptNet." <a href="https://github.com/kjunelee/MetaOptNet">https://github.com/kjunelee/MetaOptNet</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FC100</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskDataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="cifarfs">CIFARFS<a class="headerlink" href="#cifarfs" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">CIFARFS</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/cifarfs.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019.</p>
<p>It consists of 60'000 colour images of sizes 32x32 pixels.
The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.
The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019.</p>
<p><strong>References</strong></p>
<ol>
<li>Bertinetto et al. 2019. "Meta-learning with differentiable closed-form solvers". ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFARFS</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskGenerator</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">ways</span><span class="o">=</span><span class="n">ways</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="vggflower102">VGGFlower102<a class="headerlink" href="#vggflower102" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">VGGFlower102</span><span class="p">(</span><span class="n">root</span><span class="p">,</span>
             <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
             <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/vgg_flowers.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.</p>
<p>The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images.
We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al.</p>
<p><strong>References</strong></p>
<ol>
<li>Nilsback, M. and A. Zisserman. 2006. "A Visual Vocabulary for Flower Classification." CVPR '06.</li>
<li>Triantafillou et al. 2019. "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples." ICLR '20.</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/">https://www.robots.ox.ac.uk/~vgg/data/flowers/</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">VGGFlower102</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskDataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h2 id="fgvcaircraft">FGVCAircraft<a class="headerlink" href="#fgvcaircraft" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">FGVCAircraft</span><span class="p">(</span><span class="n">root</span><span class="p">,</span>
             <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">,</span>
             <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">target_transform</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fgvc_aircraft.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.</p>
<p>The dataset consists of 10,200 images of aircraft (102 classes, each 100 images).
We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al.</p>
<p><strong>References</strong></p>
<ol>
<li>Maji et al. 2013. "Fine-Grained Visual Classification of Aircraft." arXiv [cs.CV].</li>
<li>Triantafillou et al. 2019. "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples." ICLR '20.</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/">http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">vision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FGVCAircraft</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">MetaDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">l2l</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TaskDataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">num_tasks</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<h1 id="learn2learnvisiontransforms">learn2learn.vision.transforms<a class="headerlink" href="#learn2learnvisiontransforms" title="Permanent link">&para;</a></h1>
<p><strong>Description</strong></p>
<p>A set of transformations commonly used in meta-learning vision tasks.</p>
<h2 id="randomclassrotation">RandomClassRotation<a class="headerlink" href="#randomclassrotation" title="Permanent link">&para;</a></h2>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">RandomClassRotation</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">degrees</span><span class="p">)</span>
</code></pre></div>
</td></tr></table>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>Samples rotations from a given list uniformly at random, and applies it to
all images from a given class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>degrees</strong> (list) - The rotations to be sampled.</li>
</ul>
<p><strong>Example</strong></p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><code><span class="n">transform</span> <span class="o">=</span> <span class="n">RandomClassRotation</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">270</span><span class="p">])</span>
</code></pre></div>
</td></tr></table>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../learn2learn.gym/" title="learn2learn.gym" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                learn2learn.gym
              </span>
            </div>
          </a>
        
        
          <a href="../../examples.vision/" title="Computer Vision" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Computer Vision
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org" target="_blank" rel="noopener">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.c33a9706.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/mathtex-script-type.min.js"></script>
      
    
  </body>
</html>