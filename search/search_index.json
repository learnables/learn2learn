{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learn2learn is a software library for meta-learning research. learn2learn builds on top of PyTorch to accelerate two aspects of the meta-learning research cycle: fast prototyping , essential in letting researchers quickly try new ideas, and correct reproducibility , ensuring that these ideas are evaluated fairly. learn2learn provides low-level utilities and unified interface to create new algorithms and domains, together with high-quality implementations of existing algorithms and standardized benchmarks. It retains compatibility with torchvision , torchaudio , torchtext , cherry , and any other PyTorch-based library you might be using. Overview learn2learn.data : TaskDataset and transforms to create few-shot tasks from any PyTorch dataset. learn2learn.vision : Models, datasets, and benchmarks for computer vision and few-shot learning. learn2learn.gym : Environment and utilities for meta-reinforcement learning. learn2learn.algorithms : High-level wrappers for existing meta-learning algorithms. learn2learn.optim : Utilities and algorithms for differentiable optimization and meta-descent. Resources Website: http://learn2learn.net/ Documentation: http://learn2learn.net/docs/ Tutorials: http://learn2learn.net/tutorials/getting_started/ Examples: https://github.com/learnables/learn2learn/tree/master/examples GitHub: https://github.com/learnables/learn2learn/ Twitter: https://twitter.com/metalearn2learn Slack: http://slack.learn2learn.net/ Installation \u00b6 1 pip install learn2learn Snippets & Examples \u00b6 The following snippets provide a sneak peek at the functionalities of learn2learn. High-level Wrappers \u00b6 Few-Shot Learning with MAML For more algorithms (ProtoNets, ANIL, Meta-SGD, Reptile, Meta-Curvature, KFO) refer to the examples folder. Most of them can be implemented with with the GBML wrapper. ( documentation ). 1 2 3 4 5 6 7 8 9 10 maml = l2l . algorithms . MAML ( model , lr = 0.1 ) opt = torch . optim . SGD ( maml . parameters (), lr = 0.001 ) for iteration in range ( 10 ): opt . zero_grad () task_model = maml . clone () # torch.clone() for nn.Modules adaptation_loss = compute_loss ( task_model ) task_model . adapt ( adaptation_loss ) # computes gradient, update task_model in-place evaluation_loss = compute_loss ( task_model ) evaluation_loss . backward () # gradients w.r.t. maml.parameters() opt . step () Meta-Descent with Hypergradient Learn any kind of optimization algorithm with the LearnableOptimizer . ( example and documentation ) 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( l2l . nn . Scale ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) # metaopt has .step() opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) # metaopt also has .parameters() metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear Learning Domains \u00b6 Custom Few-Shot Dataset Many standardized datasets (Omniglot, mini-/tiered-ImageNet, FC100, CIFAR-FS) are readily available in learn2learn.vision.datasets . ( documentation ) 1 2 3 4 5 6 7 8 9 10 dataset = l2l . data . MetaDataset ( MyDataset ()) # any PyTorch dataset transforms = [ # Easy to define your own transform l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task # Meta-train on the task Environments and Utilities for Meta-RL Parallelize your own meta-environments with AsyncVectorEnv , or use the standardized ones. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 def make_env (): env = l2l . gym . HalfCheetahForwardBackwardEnv () env = cherry . envs . ActionSpaceScaler ( env ) return env env = l2l . gym . AsyncVectorEnv ([ make_env for _ in range ( 16 )]) # uses 16 threads for task_config in env . sample_tasks ( 20 ): env . set_task ( task ) # all threads receive the same task state = env . reset () # use standard Gym API action = my_policy ( env ) env . step ( action ) Low-Level Utilities \u00b6 Differentiable Optimization Learn and differentiate through updates of PyTorch Modules. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 12 13 model = MyModel () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) learned_update = l2l . optim . ParameterUpdate ( # learnable update function model . parameters (), transform ) clone = l2l . clone_module ( model ) # torch.clone() for nn.Modules error = loss ( clone ( X ), y ) updates = learned_update ( # similar API as torch.autograd.grad error , clone . parameters (), create_graph = True , ) l2l . update_module ( clone , updates = updates ) loss ( clone ( X ), y ) . backward () # Gradients w.r.t model.parameters() and learned_update.parameters() Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Citation \u00b6 To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} } Acknowledgements & Friends \u00b6 The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on datasets for supervised meta-learning. higher is a PyTorch library that enables differentiating through optimization inner-loops. While they monkey-patch nn.Module to be stateless, learn2learn retains the stateful PyTorch look-and-feel. For more information, refer to their ArXiv paper .","title":"Home"},{"location":"#installation","text":"1 pip install learn2learn","title":"Installation"},{"location":"#snippets-examples","text":"The following snippets provide a sneak peek at the functionalities of learn2learn.","title":"Snippets &amp; Examples"},{"location":"#high-level-wrappers","text":"Few-Shot Learning with MAML For more algorithms (ProtoNets, ANIL, Meta-SGD, Reptile, Meta-Curvature, KFO) refer to the examples folder. Most of them can be implemented with with the GBML wrapper. ( documentation ). 1 2 3 4 5 6 7 8 9 10 maml = l2l . algorithms . MAML ( model , lr = 0.1 ) opt = torch . optim . SGD ( maml . parameters (), lr = 0.001 ) for iteration in range ( 10 ): opt . zero_grad () task_model = maml . clone () # torch.clone() for nn.Modules adaptation_loss = compute_loss ( task_model ) task_model . adapt ( adaptation_loss ) # computes gradient, update task_model in-place evaluation_loss = compute_loss ( task_model ) evaluation_loss . backward () # gradients w.r.t. maml.parameters() opt . step () Meta-Descent with Hypergradient Learn any kind of optimization algorithm with the LearnableOptimizer . ( example and documentation ) 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( l2l . nn . Scale ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) # metaopt has .step() opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) # metaopt also has .parameters() metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear","title":"High-level Wrappers"},{"location":"#learning-domains","text":"Custom Few-Shot Dataset Many standardized datasets (Omniglot, mini-/tiered-ImageNet, FC100, CIFAR-FS) are readily available in learn2learn.vision.datasets . ( documentation ) 1 2 3 4 5 6 7 8 9 10 dataset = l2l . data . MetaDataset ( MyDataset ()) # any PyTorch dataset transforms = [ # Easy to define your own transform l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task # Meta-train on the task Environments and Utilities for Meta-RL Parallelize your own meta-environments with AsyncVectorEnv , or use the standardized ones. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 def make_env (): env = l2l . gym . HalfCheetahForwardBackwardEnv () env = cherry . envs . ActionSpaceScaler ( env ) return env env = l2l . gym . AsyncVectorEnv ([ make_env for _ in range ( 16 )]) # uses 16 threads for task_config in env . sample_tasks ( 20 ): env . set_task ( task ) # all threads receive the same task state = env . reset () # use standard Gym API action = my_policy ( env ) env . step ( action )","title":"Learning Domains"},{"location":"#low-level-utilities","text":"Differentiable Optimization Learn and differentiate through updates of PyTorch Modules. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 12 13 model = MyModel () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) learned_update = l2l . optim . ParameterUpdate ( # learnable update function model . parameters (), transform ) clone = l2l . clone_module ( model ) # torch.clone() for nn.Modules error = loss ( clone ( X ), y ) updates = learned_update ( # similar API as torch.autograd.grad error , clone . parameters (), create_graph = True , ) l2l . update_module ( clone , updates = updates ) loss ( clone ( X ), y ) . backward () # Gradients w.r.t model.parameters() and learned_update.parameters()","title":"Low-Level Utilities"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#citation","text":"To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} }","title":"Citation"},{"location":"#acknowledgements-friends","text":"The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on datasets for supervised meta-learning. higher is a PyTorch library that enables differentiating through optimization inner-loops. While they monkey-patch nn.Module to be stateless, learn2learn retains the stateful PyTorch look-and-feel. For more information, refer to their ArXiv paper .","title":"Acknowledgements &amp; Friends"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 v0.1.2 \u00b6 Added \u00b6 New example: Meta-World example with MAML-TRPO with it's own env wrapper. (@ Kostis-S-Z ) l2l.vision.benchmarks interface. Differentiable optimization utilities in l2l.optim . (including l2l.optim.LearnableOptimizer for meta-descent) General gradient-based meta-learning wrapper in l2l.algorithms.GBML . Various nn.Modules in l2l.nn . l2l.update_module as a more general alternative to l2l.algorithms.maml_update . Changed \u00b6 Fixed \u00b6 clone_module supports non-Module objects. VGG flowers now relies on tarfile.open() instead of tarfile.TarFile(). v0.1.1 \u00b6 Added \u00b6 New tutorial: 'Feature Reuse with ANIL'. (@ewinapun) Changed \u00b6 Mujoco imports optional for docs: the import error is postponed to first method call. Fixed \u00b6 MAML() and clone_module support for RNN modules. v0.1.0.1 \u00b6 Fixed \u00b6 Remove Cython dependency when installing from PyPI and clean up package distribution. v0.1.0 \u00b6 Added \u00b6 A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples. Changed \u00b6 Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples. Fixed \u00b6 Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#v012","text":"","title":"v0.1.2"},{"location":"changelog/#added_1","text":"New example: Meta-World example with MAML-TRPO with it's own env wrapper. (@ Kostis-S-Z ) l2l.vision.benchmarks interface. Differentiable optimization utilities in l2l.optim . (including l2l.optim.LearnableOptimizer for meta-descent) General gradient-based meta-learning wrapper in l2l.algorithms.GBML . Various nn.Modules in l2l.nn . l2l.update_module as a more general alternative to l2l.algorithms.maml_update .","title":"Added"},{"location":"changelog/#changed_1","text":"","title":"Changed"},{"location":"changelog/#fixed_1","text":"clone_module supports non-Module objects. VGG flowers now relies on tarfile.open() instead of tarfile.TarFile().","title":"Fixed"},{"location":"changelog/#v011","text":"","title":"v0.1.1"},{"location":"changelog/#added_2","text":"New tutorial: 'Feature Reuse with ANIL'. (@ewinapun)","title":"Added"},{"location":"changelog/#changed_2","text":"Mujoco imports optional for docs: the import error is postponed to first method call.","title":"Changed"},{"location":"changelog/#fixed_2","text":"MAML() and clone_module support for RNN modules.","title":"Fixed"},{"location":"changelog/#v0101","text":"","title":"v0.1.0.1"},{"location":"changelog/#fixed_3","text":"Remove Cython dependency when installing from PyPI and clean up package distribution.","title":"Fixed"},{"location":"changelog/#v010","text":"","title":"v0.1.0"},{"location":"changelog/#added_3","text":"A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples.","title":"Added"},{"location":"changelog/#changed_3","text":"Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples.","title":"Changed"},{"location":"changelog/#fixed_4","text":"Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Fixed"},{"location":"examples.optim/","text":"Meta-Optimization \u00b6 This directory contains examples of using learn2learn for meta-optimization or meta-descent. Hypergradient \u00b6 The script hypergrad_mnist.py demonstrates how to implement a slightly modified version of \" Online Learning Rate Adaptation with Hypergradient Descent \". The implementation departs from the algorithm presented in the paper in two ways. We forgo the analytical formulation of the learning rate's gradient to demonstrate the capability of the LearnableOptimizer class. We adapt per-parameter learning rates instead of updating a single learning rate shared by all parameters. Usage Warning The parameters for this script were not carefully tuned. Manually edit the script and run: 1 python examples/optimization/hypergrad_mnist.py","title":"Optimization"},{"location":"examples.optim/#meta-optimization","text":"This directory contains examples of using learn2learn for meta-optimization or meta-descent.","title":"Meta-Optimization"},{"location":"examples.optim/#hypergradient","text":"The script hypergrad_mnist.py demonstrates how to implement a slightly modified version of \" Online Learning Rate Adaptation with Hypergradient Descent \". The implementation departs from the algorithm presented in the paper in two ways. We forgo the analytical formulation of the learning rate's gradient to demonstrate the capability of the LearnableOptimizer class. We adapt per-parameter learning rates instead of updating a single learning rate shared by all parameters. Usage Warning The parameters for this script were not carefully tuned. Manually edit the script and run: 1 python examples/optimization/hypergrad_mnist.py","title":"Hypergradient"},{"location":"examples.rl/","text":"Meta-Reinforcement Learning \u00b6 Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn. MAML \u00b6 The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"Reinforcement Learning"},{"location":"examples.rl/#meta-reinforcement-learning","text":"Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn.","title":"Meta-Reinforcement Learning"},{"location":"examples.rl/#maml","text":"The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"MAML"},{"location":"examples.vision/","text":"Meta-Learning & Computer Vision \u00b6 This directory contains meta-learning examples and reproductions for common computer vision benchmarks. MAML \u00b6 The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py Prototypical Networks \u00b6 The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20 ANIL \u00b6 The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py Reptile \u00b6 The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Computer Vision"},{"location":"examples.vision/#meta-learning-computer-vision","text":"This directory contains meta-learning examples and reproductions for common computer vision benchmarks.","title":"Meta-Learning &amp; Computer Vision"},{"location":"examples.vision/#maml","text":"The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py","title":"MAML"},{"location":"examples.vision/#prototypical-networks","text":"The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20","title":"Prototypical Networks"},{"location":"examples.vision/#anil","text":"The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py","title":"ANIL"},{"location":"examples.vision/#reptile","text":"The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Reptile"},{"location":"paper_list/","text":"Paper List \u00b6 The following papers were announced on the learn2learn Twitter account . You can submit unannounced and meta-learning related papers through the following Google Form. (It does not matter if they are old or new, but they shouldn't be already announced.) Info Announce any paper via the Google Form to announce papers , also available below. Submitted Papers \u00b6 Discovering Reinforcement Learning Algorithms by Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M and Xu, Zhongwen and van Hasselt, Hado and Singh, Satinder and Silver, David http://arxiv.org/abs/2007.08794 On the Outsized Importance of Learning Rates in Local Update Methods by Charles, Zachary and Kone{\\v c}n{\\'y}, Jakub http://arxiv.org/abs/2007.00878 Global Convergence and Induced Kernels of Gradient-Based Meta-Learning with Neural Nets by Wang, Haoxiang and Sun, Ruoyu and Li, Bo http://arxiv.org/abs/2006.14606 On the Iteration Complexity of Hypergradient Computation by Grazzi, Riccardo and Franceschi, Luca and Pontil, Massimiliano and Salzo, Saverio http://arxiv.org/abs/2006.16218 On the Outsized Importance of Learning Rates in Local Update Methods by Charles, Zachary and Kone{\\v c}n{\\'y}, Jakub http://arxiv.org/abs/2007.00878 Meta-SAC: Auto-tune the Entropy Temperature of Soft Actor-Critic via Metagradient by Wang, Yufei and Ni, Tianwei http://arxiv.org/abs/2007.01932 Meta Learning in the Continuous Time Limit by Xu, Ruitu and Chen, Lin and Karbasi, Amin http://arxiv.org/abs/2006.10921 Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification by Zhou, Yucan and Wang, Yu and Cai, Jianfei and Zhou, Yu and Hu, Qinghua and Wang, Weiping http://arxiv.org/abs/2007.06240 MTL2L: A Context Aware Neural Optimiser by Kuo, Nicholas I-Hsien and Harandi, Mehrtash and Fourrier, Nicolas and Walder, Christian and Ferraro, Gabriela and Suominen, Hanna http://arxiv.org/abs/2007.09343 Navigating the Trade-Off between Multi-Task Learning and Learning to Multitask in Deep Neural Networks by Ravi, Sachin and Musslick, Sebastian and Hamin, Maia and Willke, Theodore L and Cohen, Jonathan D http://arxiv.org/abs/2007.10527 Balanced Meta-Softmax for Long-Tailed Visual Recognition by Ren, Jiawei and Yu, Cunjun and Sheng, Shunan and Ma, Xiao and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng http://arxiv.org/abs/2007.10740 CrossTransformers: spatially-aware few-shot transfer by Doersch, Carl and Gupta, Ankush and Zisserman, Andrew http://arxiv.org/abs/2007.11498 Meta-Learning a Dynamical Language Model by Wolf, Thomas and Chaumond, Julien and Delangue, Clement http://arxiv.org/abs/1803.10631 Meta-Learning Requires Meta-Augmentation by Rajendran, Janarthanan and Irpan, Alex and Jang, Eric http://arxiv.org/abs/2007.05549 Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift by Zhang, Marvin and Marklund, Henrik and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/2007.02931 Meta-Learning Symmetries by Reparameterization by Zhou, Allan and Knowles, Tom and Finn, Chelsea http://arxiv.org/abs/2007.02933 Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift by Zhang, Marvin and Marklund, Henrik and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/2007.02931 A Brief Look at Generalization in Visual Meta-Reinforcement Learning by Alver, Safa and Precup, Doina http://arxiv.org/abs/2006.07262 Learning Representations by Stochastic Meta-Gradient Descent in Neural Networks by Veeriah, Vivek and Zhang, Shangtong and Sutton, Richard S http://arxiv.org/abs/1612.02879 PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees by Rothfuss, Jonas and Fortuin, Vincent and Krause, Andreas http://arxiv.org/abs/2002.05551 Meta-Meta-Classification for One-Shot Learning by Chowdhury, Arkabandhu and Chaudhari, Dipak and Chaudhuri, Swarat and Jermaine, Chris http://arxiv.org/abs/2004.08083 Relatedness Measures to Aid the Transfer of Building Blocks among Multiple Tasks by Nguyen, Trung B and Browne, Will N and Zhang, Mengjie http://arxiv.org/abs/2005.03947 Information-Theoretic Generalization Bounds for Meta-Learning and Applications by Jose, Sharu Theresa and Simeone, Osvaldo http://arxiv.org/abs/2005.04372 On Learning Intrinsic Rewards for Policy Gradient Methods by Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder http://arxiv.org/abs/1804.06459 A Sample Complexity Separation between Non-Convex and Convex Meta-Learning by Saunshi, Nikunj and Zhang, Yi and Khodak, Mikhail and Arora, Sanjeev http://arxiv.org/abs/2002.11172 Bayesian Online Meta-Learning with Laplace Approximation by Yap, Pau Ching and Ritter, Hippolyt and Barber, David http://arxiv.org/abs/2005.00146 Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks by Schoettler, Gerrit and Nair, Ashvin and Ojea, Juan Aparicio and Levine, Sergey and Solowjow, Eugen http://arxiv.org/abs/2004.14404 Continual Deep Learning by Functional Regularisation of Memorable Past by Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E and Khan, Mohammad Emtiyaz http://arxiv.org/abs/2004.14070 Jelly Bean World: A Testbed for Never-Ending Learning by Platanios, Emmanouil Antonios and Saparov, Abulhair and Mitchell, Tom https://openreview.net/pdf?id=Byx_YAVYPH Encouraging behavioral diversity in evolutionary robotics: an empirical study by Mouret, J-B and Doncieux, S http://dx.doi.org/10.1162/EVCO_a_00048 Defining Benchmarks for Continual Few-Shot Learning by Antoniou, Antreas and Patacchiola, Massimiliano and Ochal, Mateusz and Storkey, Amos http://arxiv.org/abs/2004.11967 Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning by Sharma, Archit and Ahn, Michael and Levine, Sergey and Kumar, Vikash and Hausman, Karol and Gu, Shixiang http://arxiv.org/abs/2004.12974 Empirical Bayes Transductive Meta-Learning with Synthetic Gradients by Hu, Shell Xu and Moreno, Pablo G and Xiao, Yang and Shen, Xi and Obozinski, Guillaume and Lawrence, Neil D and Damianou, Andreas http://arxiv.org/abs/2004.12696 Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems by Ben-Iwhiwhu, Eseoghene and Ladosz, Pawel and Dick, Jeffery and Chen, Wen-Hua and Pilly, Praveen and Soltoggio, Andrea http://arxiv.org/abs/2004.12846 Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning by Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey http://arxiv.org/abs/1910.10897 Meta reinforcement learning as task inference by Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A and Teh, Yee Whye and Heess, Nicolas http://arxiv.org/abs/1905.06424 Meta-Gradient Reinforcement Learning by Xu, Zhongwen and van Hasselt, Hado and Silver, David http://arxiv.org/abs/1805.09801 Self-Paced Deep Reinforcement Learning by Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni http://arxiv.org/abs/2004.11812 Scheduling the Learning Rate Via Hypergradients: New Insights and a New Algorithm by Donini, Michele and Franceschi, Luca and Majumder, Orchid and Pontil, Massimiliano and Frasconi, Paolo https://openreview.net/pdf?id=Ske6qJSKPH Learning Stabilizable Nonlinear Dynamics with Contraction-Based Regularization by Singh, Sumeet and Richards, Spencer M and Sindhwani, Vikas and Slotine, Jean-Jacques E and Pavone, Marco http://arxiv.org/abs/1907.13122 A Comprehensive Overview and Survey of Recent Advances in Meta-Learning by Peng, Huimin http://arxiv.org/abs/2004.11149 Learning a Formula of Interpretability to Learn Interpretable Formulas by Virgolin, Marco and De Lorenzo, Andrea and Medvet, Eric and Randone, Francesca http://arxiv.org/abs/2004.11170 Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads by Belkhale, Suneel and Li, Rachel and Kahn, Gregory and McAllister, Rowan and Calandra, Roberto and Levine, Sergey http://arxiv.org/abs/2004.11345 Frustratingly Simple Few-Shot Object Detection by Wang, Xin and Huang, Thomas E and Darrell, Trevor and Gonzalez, Joseph E and Yu, Fisher http://arxiv.org/abs/2003.06957 Meta Pseudo Labels by Pham, Hieu and Xie, Qizhe and Dai, Zihang and Le, Quoc V http://arxiv.org/abs/2003.10580 0e56da12-a2f0-4288-b745-c15deec9183a by Unknown http://learn2learn.net Finding online neural update rules by learning to remember by Gregor, Karol http://arxiv.org/abs/2003.03124 A New Meta-Baseline for Few-Shot Learning by Chen, Yinbo and Wang, Xiaolong and Liu, Zhuang and Xu, Huijuan and Darrell, Trevor http://arxiv.org/abs/2003.04390 Learning to be Global Optimizer by Zhang, Haotian and Sun, Jianyong and Xu, Zongben http://arxiv.org/abs/2003.04521 Scalable Multi-Task Imitation Learning with Autonomous Improvement by Singh, Avi and Jang, Eric and Irpan, Alexander and Kappler, Daniel and Dalal, Murtaza and Levine, Sergey and Khansari, Mohi and Finn, Chelsea http://arxiv.org/abs/2003.02636 Meta-learning for mixed linear regression by Kong, Weihao and Somani, Raghav and Song, Zhao and Kakade, Sham and Oh, Sewoong http://arxiv.org/abs/2002.08936 Provable Meta-Learning of Linear Representations by Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael I http://arxiv.org/abs/2002.11684 Learning to Continually Learn by Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff and Cheney, Nick http://arxiv.org/abs/2002.09571 PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees by Rothfuss, Jonas and Fortuin, Vincent and Krause, Andreas http://arxiv.org/abs/2002.05551 Incremental Learning for Metric-Based Meta-Learners by Liu, Qing and Majumder, Orchid and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano http://arxiv.org/abs/2002.04162 Hyper-Meta Reinforcement Learning with Sparse Reward by Hua, Yun and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Yan, Junchi and He, Xiaofeng and Zha, Hongyuan http://arxiv.org/abs/2002.04238 Meta-Learning across Meta-Tasks for Few-Shot Learning by Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Tian, Jia and Xiang, Tao and Wen, Ji-Rong http://arxiv.org/abs/2002.04274 Distribution-Agnostic Model-Agnostic Meta-Learning by Collins, Liam and Mokhtari, Aryan and Shakkottai, Sanjay http://arxiv.org/abs/2002.04766 Provably Convergent Policy Gradient Methods for Model-Agnostic Meta-Reinforcement Learning by Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman http://arxiv.org/abs/2002.05135 Meta-learning framework with applications to zero-shot time-series forecasting by Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua http://arxiv.org/abs/2002.02887 A Loss-Function for Causal Machine-Learning by Yang, I-Sheng http://arxiv.org/abs/2001.00629 Self-Tuning Deep Reinforcement Learning by Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Van Hasslet, Hado and Silver, David and Singh, Satinder http://arxiv.org/abs/2002.12928 Learning Adaptive Loss for Robust Learning with Noisy Labels by Shu, Jun and Zhao, Qian and Chen, Keyu and Xu, Zongben and Meng, Deyu http://arxiv.org/abs/2002.06482 A Structured Prediction Approach for Conditional Meta-Learning by Wang, Ruohan and Demiris, Yiannis and Ciliberto, Carlo http://arxiv.org/abs/2002.08799 Curriculum in Gradient-Based Meta-Reinforcement Learning by Mehta, Bhairav and Deleu, Tristan and Raparthy, Sharath Chandra and Pal, Chris J and Paull, Liam http://arxiv.org/abs/2002.07956 Multi-Step Model-Agnostic Meta-Learning: Convergence and Improved Algorithms by Ji, Kaiyi and Yang, Junjie and Liang, Yingbin http://arxiv.org/abs/2002.07836 Local Nonparametric Meta-Learning by Goo, Wonjoon and Niekum, Scott http://arxiv.org/abs/2002.03272 Revisiting Meta-Learning as Supervised Learning by Chao, Wei-Lun and Ye, Han-Jia and Zhan, De-Chuan and Campbell, Mark and Weinberger, Kilian Q http://arxiv.org/abs/2002.00573 SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning by Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and van der Maaten, Laurens http://arxiv.org/abs/1911.04623 Fast and Generalized Adaptation for Few-Shot Learning by Song, Liang and Liu, Jinlu and Qin, Yongqiang http://arxiv.org/abs/1911.10807 Meta-Learning without Memorization by Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1912.03820 Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One by Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\\\"o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin http://arxiv.org/abs/1912.03263 MAME : Model-Agnostic Meta-Exploration by Gurumurthy, Swaminathan and Kumar, Sumit and Sycara, Katia http://arxiv.org/abs/1911.04024 Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features by Gui, Tao and Qing, Lizhi and Zhang, Qi and Ye, Jiacheng and Yan, Hang and Fei, Zichu and Huang, Xuanjing http://arxiv.org/abs/1911.07518 Meta Adaptation using Importance Weighted Demonstrations by Lekkala, Kiran and Abu-El-Haija, Sami and Itti, Laurent http://arxiv.org/abs/1911.10322 VIABLE: Fast Adaptation via Backpropagating Learned Loss by Feng, Leo and Zintgraf, Luisa and Peng, Bei and Whiteson, Shimon http://arxiv.org/abs/1911.13159 Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning by Arnold, S{\\'e}bastien M R and Iqbal, Shariq and Sha, Fei http://arxiv.org/abs/1910.13603 TADAM: Task dependent adaptive metric for improved few-shot learning by Oreshkin, Boris and Rodr{\\'\\i}guez L{\\'o}pez, Pau and Lacoste, Alexandre http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks by Bansal, Trapit and Jha, Rishikesh and McCallum, Andrew http://arxiv.org/abs/1911.03863 Optimizing Millions of Hyperparameters by Implicit Differentiation by Lorraine, Jonathan and Vicol, Paul and Duvenaud, David http://arxiv.org/abs/1911.02590 Meta-data: Characterization of Input Features for Meta-learning by Castiello, Ciro and Castellano, Giovanna and Fanelli, Anna Maria http://dx.doi.org/10.1007/11526018_45 Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems by Mi, Fei and Huang, Minlie and Zhang, Jiyong and Faltings, Boi http://arxiv.org/abs/1905.05644 Domain Generalization via Model-Agnostic Learning of Semantic Features by Dou, Qi and Castro, Daniel C and Kamnitsas, Konstantinos and Glocker, Ben http://arxiv.org/abs/1910.13580 Hierarchical Expert Networks for Meta-Learning by Hihn, Heinke and Braun, Daniel A http://arxiv.org/abs/1911.00348 Online Meta-Learning on Non-convex Setting by Zhuang, Zhenxun and Wang, Yunlong and Yu, Kezi and Lu, Songtao http://arxiv.org/abs/1910.10196 Learning-to-Learn Stochastic Gradient Descent with Biased Regularization by Denevi, Giulia and Ciliberto, Carlo and Grazzi, Riccardo and Pontil, Massimiliano http://arxiv.org/abs/1903.10399 Provable Guarantees for Gradient-Based Meta-Learning by Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet http://arxiv.org/abs/1902.10644 The TCGA Meta-Dataset Clinical Benchmark by Samiei, Mandana and W{\\\"u}rfl, Tobias and Deleu, Tristan and Weiss, Martin and Dutil, Francis and Fevens, Thomas and Boucher, Genevi{`e}ve and Lemieux, Sebastien and Cohen, Joseph Paul http://arxiv.org/abs/1910.08636 VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning by Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon http://arxiv.org/abs/1910.08348 Meta-Transfer Learning through Hard Tasks by Sun, Qianru and Liu, Yaoyao and Chen, Zhaozheng and Chua, Tat-Seng and Schiele, Bernt http://arxiv.org/abs/1910.03648 Model-Agnostic Meta-Learning using Runge-Kutta Methods by Im, Daniel Jiwoong and Jiang, Yibo and Verma, Nakul http://arxiv.org/abs/1910.07368 Improving Generalization in Meta Reinforcement Learning using Learned Objectives by Kirsch, Louis and van Steenkiste, Sjoerd and Schmidhuber, J{\\\"u}rgen http://arxiv.org/abs/1910.04098 Generalized Inner Loop Meta-Learning by Grefenstette, Edward and Amos, Brandon and Yarats, Denis and Htut, Phu Mon and Molchanov, Artem and Meier, Franziska and Kiela, Douwe and Cho, Kyunghyun and Chintala, Soumith http://arxiv.org/abs/1910.01727 Is Fast Adaptation All You Need? by Javed, Khurram and Yao, Hengshuai and White, Martha http://arxiv.org/abs/1910.01705 Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in Damaged Robots by Verma, Shresth and Nair, Haritha S and Agarwal, Gaurav and Dhar, Joydip and Shukla, Anupam http://arxiv.org/abs/1910.01240 ES-MAML: Simple Hessian-Free Meta Learning by Song, Xingyou and Gao, Wenbo and Yang, Yuxiang and Choromanski, Krzysztof and Pacchiano, Aldo and Tang, Yunhao http://arxiv.org/abs/1910.01215 Meta-Q-Learning by Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J http://arxiv.org/abs/1910.00125 Efficient meta reinforcement learning via meta goal generation by Fu, Haotian and Tang, Hongyao and Hao, Jianye http://arxiv.org/abs/1909.13607 Chameleon: Learning Model Initializations Across Tasks With Different Schemas by Brinkmeyer, Lukas and Drumond, Rafael Rego and Scholz, Randolf and Grabocka, Josif and Schmidt-Thieme, Lars http://arxiv.org/abs/1909.13576 Learning Fast Adaptation with Meta Strategy Optimization by Yu, Wenhao and Tan, Jie and Bai, Yunfei and Coumans, Erwin and Ha, Sehoon http://arxiv.org/abs/1909.12995 Meta-Inverse Reinforcement Learning with Probabilistic Context Variables by Yu, Lantao and Yu, Tianhe and Finn, Chelsea and Ermon, Stefano http://arxiv.org/abs/1909.09314 Modular Meta-Learning with Shrinkage by Chen, Yutian and Friesen, Abram L and Behbahani, Feryal and Budden, David and Hoffman, Matthew W and Doucet, Arnaud and de Freitas, Nando http://arxiv.org/abs/1909.05557 Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning by Farquhar, Gregory and Whiteson, Shimon and Foerster, Jakob http://arxiv.org/abs/1909.10549 Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML by Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol http://arxiv.org/abs/1909.09157 Meta-Learning by Vanschoren, Joaquin https://doi.org/10.1007/978-3-030-05318-5_2 Understanding Short-Horizon Bias in Stochastic Meta-Optimization by Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger http://arxiv.org/abs/1803.02021 On First-Order Meta-Learning Algorithms by Nichol, Alex and Achiam, Joshua and Schulman, John http://arxiv.org/abs/1803.02999 Towards Understanding Generalization in Gradient-Based Meta-Learning by Guiroy, Simon and Verma, Vikas and Pal, Christopher http://arxiv.org/abs/1907.07287 They empirically study the landscape of fast-adaptation in MAML. The most interesting claim is that when meta-overfitting, the loss landscape becomes flatter on test tasks. On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms by Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman http://arxiv.org/abs/1908.10400 Learning to Learn with Gradients by Finn, Chelsea http://learn2learn.net Acetylcholine and memory by Hasselmo, M E and Bower, J M https://www.ncbi.nlm.nih.gov/pubmed/7688162 A THEORY OF META-LEARNING AND PRINCIPLES OF FACILITATION: AN ORGANISMIC PERSPECTIVE by Maudsley, Donald B https://uosc.primo.exlibrisgroup.com/discovery/fulldisplay?docid=proquest302999651&context=PC&vid=01USC_INST:01USC&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&mode=Basic THE ROLE OF METALEARNING IN STUDY PROCESSES by Biggs, J B http://doi.wiley.com/10.1111/j.2044-8279.1985.tb02625.x Understanding and correcting pathologies in the training of learned optimizers by Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Daniel Freeman, C and Sohl-Dickstein, Jascha http://arxiv.org/abs/1810.10180 Provides many tricks (e.g. split train batch for model \\& opt, average gradient estimators) for training differentiable optimizers online. They also have a couple of interesting observations specific to recurrent optimizers. Learned Optimizers that Scale and Generalize by Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha http://arxiv.org/abs/1703.04813 Using learned optimizers to make models robust to input noise by Metz, Luke and Maheswaranathan, Niru and Shlens, Jonathon and Sohl-Dickstein, Jascha and Cubuk, Ekin D http://arxiv.org/abs/1906.03367 Learning to Optimize Neural Nets by Li, Ke and Malik, Jitendra http://arxiv.org/abs/1703.00441 Meta-Learning Update Rules for Unsupervised Representation Learning by Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha http://arxiv.org/abs/1804.00222 Learning to Optimize by Li, Ke and Malik, Jitendra http://arxiv.org/abs/1606.01885 Learning to learn by gradient descent by gradient descent by Andrychowicz, M and Denil, M and Gomez, S http://learn2learn.net Online Learning Rate Adaptation with Hypergradient Descent by Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank http://arxiv.org/abs/1703.04782 They adapt the learning rate of SGD by differentiating the loss of the next parameters w.r.t. the learning rate. They observe that the gradient of the learning rate is simply the inner product of the last two gradients. Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta by Sutton, Richard S http://dx.doi.org/ What's mostly interesting in this paper is the adaptation of delta-bar-delta to the online scenario. The idea of representing the learning rate as an exponential is nice. Also nice to see that the derivation suggests a full-matrix adaptive case. Gain adaptation beats least squares by Sutton, Richard S https://pdfs.semanticscholar.org/7ec8/876f219b3b3d5c894a3f395c89c382029cc5.pdf This paper extends IDBD as algorithms K1 and K2, but from my quick read, it isn't clear what's the motivation for those modifications. (Seems to work in a `normalized space'', {\\ a} la natural gradient ?)They do work better. Local Gain Adaptation in Stochastic Gradient Descent by Schraudolph, Nicol N https://pdfs.semanticscholar.org/31a0/b86c3cd04e6539626f34b80db7ff79d23f40.pdf This algorithm extends IDBD (Sutton) to the non-linear setting. Interestingly, they have a few brief discussionson the difficulties to optimize at the meta-level. (c.f. Meta-level conditioning section.) Overall, it shines light on the ground idea behind IDBD. TIDBD: Adapting Temporal-difference Step-sizes Through Stochastic Meta-descent by Kearney, Alex and Veeriah, Vivek and Travnik, Jaden B and Sutton, Richard S and Pilarski, Patrick M http://arxiv.org/abs/1804.03334 Increased rates of convergence through learning rate adaptation by Jacobs, Robert A http://www.sciencedirect.com/science/article/pii/0893608088900032 This paper argues that we need (at least) four ingredients to improve optimization of connectionist networks: 1. each parameter has its own stepsize, 2. stepsizes vary over time, 3. if consecutive gradients of a stepsize have the same sign, the stepsize should be increased, 4. conversely, if the stepsize should be decreased if its gradients have opposite signs. It also proposes to use two improvements: 1. Momentum (i.e. Polyak's heavyball), 2. delta-bar-delta (i.e. learning the stepsize). It has an interesting comment on the difficulty of learning the stepsize, and therefore comes up with a ``hack'' that outperforms momentum. Meta-descent for Online, Continual Prediction by Jacobsen, Andrew and Schlegel, Matthew and Linke, Cameron and Degris, Thomas and White, Adam and White, Martha http://arxiv.org/abs/1907.07751 The idea is to learn the learning rate so as to minimize the norm of the gradient. They argue that for the continual learning setting, this forces the algorithm to stay ``as stable as possible''. No theorems, small-scale (but interesting) experiments. Adaptation of learning rate parameters by Sutton, Rich http://learn2learn.net Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace by Lee, Yoonho and Choi, Seungjin http://arxiv.org/abs/1801.05558 Meta-Learning with Warped Gradient Descent by Flennerhag, Sebastian and Rusu, Andrei A and Pascanu, Razvan and Yin, Hujun and Hadsell, Raia http://arxiv.org/abs/1909.00025 Meta-Learning via Learned Loss by Chebotar, Yevgen and Molchanov, Artem and Bechtle, Sarah and Righetti, Ludovic and Meier, Franziska and Sukhatme, Gaurav http://arxiv.org/abs/1906.05374 They learn the loss as a NN, and that loss's objective is to maximize the sum of rewards. It is provided a bunch of things, including inputs, outputs, goals. Meta-Curvature by Park, Eunbyung and Oliva, Junier B http://arxiv.org/abs/1902.03356 Alpha MAML: Adaptive Model-Agnostic Meta-Learning by Behl, Harkirat Singh and Baydin, At{\\i}l{\\i}m G{\\\"u}ne{\\c s} and Torr, Philip H S http://arxiv.org/abs/1905.07435 They combine hypergradient and MAML: adapt all learning rates at all times. Meta-SGD: Learning to Learn Quickly for Few-Shot Learning by Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang http://arxiv.org/abs/1707.09835 ProMP: Proximal Meta-Policy Search by Rothfuss, Jonas and Lee, Dennis and Clavera, Ignasi and Asfour, Tamim and Abbeel, Pieter http://arxiv.org/abs/1810.06784 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks by Finn, Chelsea and Abbeel, Pieter and Levine, Sergey http://learn2learn.net Optimization as a model for few-shot learning by Ravi, Sachin and Larochelle, Hugo https://openreview.net/pdf?id=rJY0-Kcll Fast Context Adaptation via Meta-Learning by Zintgraf, Luisa M and Shiarlis, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon http://arxiv.org/abs/1810.03642 Meta-Learning with Implicit Gradients by Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey http://arxiv.org/abs/1909.04630 Natural Neural Networks by Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray http://dl.acm.org/citation.cfm?id=2969442.2969471 A Baseline for Few-Shot Image Classification by Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano http://arxiv.org/abs/1909.02729 A CLOSER LOOK AT FEW-SHOT CLASSIFICATION by Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt https://openreview.net/pdf?id=HkxLXnAcFQ Suggests that meta-learning papers haven't been tested against classical baselines. When considering those baselines, they perform better than many of the recent meta-learning techniques. Meta-learning with differentiable closed-form solvers by Bertinetto, Luca and Henriques, Joao F and Torr, Philip and Vedaldi, Andrea https://openreview.net/forum?id=HyxnZh0ct7 Uncertainty in Model-Agnostic Meta-Learning using Variational Inference by Nguyen, Cuong and Do, Thanh-Toan and Carneiro, Gustavo http://arxiv.org/abs/1907.11864 Meta-Reinforcement Learning of Structured Exploration Strategies by Gupta, Abhishek and Mendonca, Russell and Liu, Yuxuan and Abbeel, Pieter and Levine, Sergey http://arxiv.org/abs/1802.07245 Metalearned Neural Memory by Munkhdalai, Tsendsuren and Sordoni, Alessandro and Wang, Tong and Trischler, Adam http://arxiv.org/abs/1907.09720 Accelerated Stochastic Approximation by Kesten, Harry https://projecteuclid.org/euclid.aoms/1177706705 Meta-Learning for Black-box Optimization by Vishnu, T V and Malhotra, Pankaj and Narwariya, Jyoti and Vig, Lovekesh and Shroff, Gautam http://arxiv.org/abs/1907.06901 They essentially extend the recurrent meta-learning framework in a few ways: 1. Use regret instead of objective improvement as meta-learning objective. 2. Normalize the objective so as to make it play nice with LSTMs. 3. Incorporate domain-constraints, so that the LSTM always outputs feasible solutions. All are described in page 3. Task Agnostic Continual Learning via Meta Learning by He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan http://arxiv.org/abs/1906.05201 Watch, Try, Learn: Meta-Learning from Demonstrations and Reward by Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1906.03352 Meta-Learning Representations for Continual Learning by Javed, Khurram and White, Martha http://arxiv.org/abs/1905.12588 TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning by Yoon, Sung Whan and Seo, Jun and Moon, Jaekyun http://arxiv.org/abs/1905.06549 Meta Reinforcement Learning with Task Embedding and Shared Policy by Lan, Lin and Li, Zhenguo and Guan, Xiaohong and Wang, Pinghui http://arxiv.org/abs/1905.06527 Hierarchically Structured Meta-learning by Yao, Huaxiu and Wei, Ying and Huang, Junzhou and Li, Zhenhui http://arxiv.org/abs/1905.05301 Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning by Hafez, Muhammad Burhan and Weber, Cornelius and Kerzel, Matthias and Wermter, Stefan http://arxiv.org/abs/1905.01718 Learning to Learn in Simulation by Teng, Ervin and Iannucci, Bob http://arxiv.org/abs/1902.01569 Meta-Learning with Differentiable Convex Optimization by Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano http://arxiv.org/abs/1904.03758 Functional Regularisation for Continual Learning by Titsias, Michalis K and Schwarz, Jonathan and de G. Matthews, Alexander G and Pascanu, Razvan and Teh, Yee Whye http://arxiv.org/abs/1901.11356 Learning to Forget for Meta-Learning by Baik, Sungyong and Hong, Seokil and Lee, Kyoung Mu http://arxiv.org/abs/1906.05895 Meta-learning of Sequential Strategies by Ortega, Pedro A and Wang, Jane X and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and Gy{\\\"o}rgy, Andr{\\'a}s and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and van Hasselt, Hado and de Freitas, Nando and Botvinick, Matthew and Legg, Shane http://arxiv.org/abs/1905.03030 This paper essentially provides a theoretical framework to ground the fact that recurrent meta-learning (RL^2, LLGD^2) performs Bayesian inference during adaptation. Auto-Meta: Automated Gradient Based Meta Learner Search by Kim, Jaehong and Lee, Sangyeul and Kim, Sungwan and Cha, Moonsu and Lee, Jung Kwon and Choi, Youngduck and Choi, Yongseok and Cho, Dong-Yeon and Kim, Jiwon http://arxiv.org/abs/1806.06927 Adaptive Gradient-Based Meta-Learning Methods by Khodak, Mikhail and Florina-Balcan, Maria and Talwalkar, Ameet http://arxiv.org/abs/1906.02717 Embedded Meta-Learning: Toward more flexible deep-learning models by Lampinen, Andrew K and McClelland, James L http://arxiv.org/abs/1905.09950 Modular meta-learning by Alet, Ferran and Lozano-P{\\'e}rez, Tom{\\'a}s and Kaelbling, Leslie P http://arxiv.org/abs/1806.10166 MetaPred: Meta-Learning for Clinical Risk Prediction with Limited Patient Electronic Health Records by Zhang, Xi Sheryl and Tang, Fengyi and Dodge, Hiroko and Zhou, Jiayu and Wang, Fei http://arxiv.org/abs/1905.03218 Prototypical Networks for Few-shot Learning by Snell, Jake and Swersky, Kevin and Zemel, Richard S http://arxiv.org/abs/1703.05175 Meta-learners' learning dynamics are unlike learners' by Rabinowitz, Neil C http://arxiv.org/abs/1905.01320 Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity by Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O https://openreview.net/forum?id=r1lrAiA5Ym Reinforcement Learning, Fast and Slow by Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis http://dx.doi.org/10.1016/j.tics.2019.02.006 Been There, Done That: Meta-Learning with Episodic Recall by Ritter, Samuel and Wang, Jane X and Kurth-Nelson, Zeb and Jayakumar, Siddhant M and Blundell, Charles and Pascanu, Razvan and Botvinick, Matthew http://arxiv.org/abs/1805.09692 Guided Meta-Policy Search by Mendonca, Russell and Gupta, Abhishek and Kralev, Rosen and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1904.00956 Hierarchical Meta Learning by Zou, Yingtian and Feng, Jiashi http://arxiv.org/abs/1904.09081 A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms by Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher http://arxiv.org/abs/1901.10912 Generalize Across Tasks: Efficient Algorithms for Linear Representation Learning by Bullins, Brian and Hazan, Elad and Kalai, Adam and Livni, Roi http://proceedings.mlr.press/v98/bullins19a.html Incremental Learning-to-Learn with Statistical Guarantees by Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano http://arxiv.org/abs/1803.08089 A Model of Inductive Bias Learning by Baxter, J http://arxiv.org/abs/1106.0245 Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables by Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey http://arxiv.org/abs/1903.08254 Continual Learning with Tiny Episodic Memories by Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip H S and Ranzato, Marc'aurelio http://arxiv.org/abs/1902.10486 Online Meta-Learning by Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey http://arxiv.org/abs/1902.08438 Modulating transfer between tasks in gradient-based meta-learning by Grant, Erin and Jerfel, Ghassen and Heller, Katherine and Griffiths, Thomas L https://openreview.net/pdf?id=HyxpNnRcFX Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning by Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1803.11347 Meta-Learning with Latent Embedding Optimization by Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia http://arxiv.org/abs/1807.05960 Learning to Generalize: Meta-Learning for Domain Generalization by Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M http://arxiv.org/abs/1710.03463 Some Considerations on Learning to Explore via Meta-Reinforcement Learning by Stadie, Bradly C and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya http://arxiv.org/abs/1803.01118 How to train your MAML by Antoniou, Antreas and Edwards, Harrison and Storkey, Amos http://arxiv.org/abs/1810.09502 Bayesian Model-Agnostic Meta-Learning by Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin http://arxiv.org/abs/1806.03836 Probabilistic Model-Agnostic Meta-Learning by Finn, Chelsea and Xu, Kelvin and Levine, Sergey http://arxiv.org/abs/1806.02817 The effects of negative adaptation in Model-Agnostic Meta-Learning by Deleu, Tristan and Bengio, Yoshua http://arxiv.org/abs/1812.02159 Memory-based Parameter Adaptation by Sprechmann, Pablo and Jayakumar, Siddhant M and Rae, Jack W and Pritzel, Alexander and Badia, Adri{`a} Puigdom{`e}nech and Uria, Benigno and Vinyals, Oriol and Hassabis, Demis and Pascanu, Razvan and Blundell, Charles http://arxiv.org/abs/1802.10542 Deep Meta-Learning: Learning to Learn in the Concept Space by Zhou, Fengwei and Wu, Bin and Li, Zhenguo http://arxiv.org/abs/1802.03596 Deep Prior by Lacoste, Alexandre and Boquet, Thomas and Rostamzadeh, Negar and Oreshkin, Boris and Chung, Wonchang and Krueger, David http://arxiv.org/abs/1712.05016 Recasting Gradient-Based Meta-Learning as Hierarchical Bayes by Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas http://arxiv.org/abs/1801.08930 WNGrad: Learn the Learning Rate in Gradient Descent by Wu, Xiaoxia and Ward, Rachel and Bottou, L{\\'e}on http://arxiv.org/abs/1803.02865 Learning to Learn by Finn, Chelsea http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/ Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments by Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter http://arxiv.org/abs/1710.03641 Submission Form \u00b6 Loading\u2026","title":"Paper List"},{"location":"paper_list/#paper-list","text":"The following papers were announced on the learn2learn Twitter account . You can submit unannounced and meta-learning related papers through the following Google Form. (It does not matter if they are old or new, but they shouldn't be already announced.) Info Announce any paper via the Google Form to announce papers , also available below.","title":"Paper List"},{"location":"paper_list/#submitted-papers","text":"Discovering Reinforcement Learning Algorithms by Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M and Xu, Zhongwen and van Hasselt, Hado and Singh, Satinder and Silver, David http://arxiv.org/abs/2007.08794 On the Outsized Importance of Learning Rates in Local Update Methods by Charles, Zachary and Kone{\\v c}n{\\'y}, Jakub http://arxiv.org/abs/2007.00878 Global Convergence and Induced Kernels of Gradient-Based Meta-Learning with Neural Nets by Wang, Haoxiang and Sun, Ruoyu and Li, Bo http://arxiv.org/abs/2006.14606 On the Iteration Complexity of Hypergradient Computation by Grazzi, Riccardo and Franceschi, Luca and Pontil, Massimiliano and Salzo, Saverio http://arxiv.org/abs/2006.16218 On the Outsized Importance of Learning Rates in Local Update Methods by Charles, Zachary and Kone{\\v c}n{\\'y}, Jakub http://arxiv.org/abs/2007.00878 Meta-SAC: Auto-tune the Entropy Temperature of Soft Actor-Critic via Metagradient by Wang, Yufei and Ni, Tianwei http://arxiv.org/abs/2007.01932 Meta Learning in the Continuous Time Limit by Xu, Ruitu and Chen, Lin and Karbasi, Amin http://arxiv.org/abs/2006.10921 Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification by Zhou, Yucan and Wang, Yu and Cai, Jianfei and Zhou, Yu and Hu, Qinghua and Wang, Weiping http://arxiv.org/abs/2007.06240 MTL2L: A Context Aware Neural Optimiser by Kuo, Nicholas I-Hsien and Harandi, Mehrtash and Fourrier, Nicolas and Walder, Christian and Ferraro, Gabriela and Suominen, Hanna http://arxiv.org/abs/2007.09343 Navigating the Trade-Off between Multi-Task Learning and Learning to Multitask in Deep Neural Networks by Ravi, Sachin and Musslick, Sebastian and Hamin, Maia and Willke, Theodore L and Cohen, Jonathan D http://arxiv.org/abs/2007.10527 Balanced Meta-Softmax for Long-Tailed Visual Recognition by Ren, Jiawei and Yu, Cunjun and Sheng, Shunan and Ma, Xiao and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng http://arxiv.org/abs/2007.10740 CrossTransformers: spatially-aware few-shot transfer by Doersch, Carl and Gupta, Ankush and Zisserman, Andrew http://arxiv.org/abs/2007.11498 Meta-Learning a Dynamical Language Model by Wolf, Thomas and Chaumond, Julien and Delangue, Clement http://arxiv.org/abs/1803.10631 Meta-Learning Requires Meta-Augmentation by Rajendran, Janarthanan and Irpan, Alex and Jang, Eric http://arxiv.org/abs/2007.05549 Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift by Zhang, Marvin and Marklund, Henrik and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/2007.02931 Meta-Learning Symmetries by Reparameterization by Zhou, Allan and Knowles, Tom and Finn, Chelsea http://arxiv.org/abs/2007.02933 Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift by Zhang, Marvin and Marklund, Henrik and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/2007.02931 A Brief Look at Generalization in Visual Meta-Reinforcement Learning by Alver, Safa and Precup, Doina http://arxiv.org/abs/2006.07262 Learning Representations by Stochastic Meta-Gradient Descent in Neural Networks by Veeriah, Vivek and Zhang, Shangtong and Sutton, Richard S http://arxiv.org/abs/1612.02879 PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees by Rothfuss, Jonas and Fortuin, Vincent and Krause, Andreas http://arxiv.org/abs/2002.05551 Meta-Meta-Classification for One-Shot Learning by Chowdhury, Arkabandhu and Chaudhari, Dipak and Chaudhuri, Swarat and Jermaine, Chris http://arxiv.org/abs/2004.08083 Relatedness Measures to Aid the Transfer of Building Blocks among Multiple Tasks by Nguyen, Trung B and Browne, Will N and Zhang, Mengjie http://arxiv.org/abs/2005.03947 Information-Theoretic Generalization Bounds for Meta-Learning and Applications by Jose, Sharu Theresa and Simeone, Osvaldo http://arxiv.org/abs/2005.04372 On Learning Intrinsic Rewards for Policy Gradient Methods by Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder http://arxiv.org/abs/1804.06459 A Sample Complexity Separation between Non-Convex and Convex Meta-Learning by Saunshi, Nikunj and Zhang, Yi and Khodak, Mikhail and Arora, Sanjeev http://arxiv.org/abs/2002.11172 Bayesian Online Meta-Learning with Laplace Approximation by Yap, Pau Ching and Ritter, Hippolyt and Barber, David http://arxiv.org/abs/2005.00146 Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks by Schoettler, Gerrit and Nair, Ashvin and Ojea, Juan Aparicio and Levine, Sergey and Solowjow, Eugen http://arxiv.org/abs/2004.14404 Continual Deep Learning by Functional Regularisation of Memorable Past by Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E and Khan, Mohammad Emtiyaz http://arxiv.org/abs/2004.14070 Jelly Bean World: A Testbed for Never-Ending Learning by Platanios, Emmanouil Antonios and Saparov, Abulhair and Mitchell, Tom https://openreview.net/pdf?id=Byx_YAVYPH Encouraging behavioral diversity in evolutionary robotics: an empirical study by Mouret, J-B and Doncieux, S http://dx.doi.org/10.1162/EVCO_a_00048 Defining Benchmarks for Continual Few-Shot Learning by Antoniou, Antreas and Patacchiola, Massimiliano and Ochal, Mateusz and Storkey, Amos http://arxiv.org/abs/2004.11967 Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning by Sharma, Archit and Ahn, Michael and Levine, Sergey and Kumar, Vikash and Hausman, Karol and Gu, Shixiang http://arxiv.org/abs/2004.12974 Empirical Bayes Transductive Meta-Learning with Synthetic Gradients by Hu, Shell Xu and Moreno, Pablo G and Xiao, Yang and Shen, Xi and Obozinski, Guillaume and Lawrence, Neil D and Damianou, Andreas http://arxiv.org/abs/2004.12696 Evolving Inborn Knowledge For Fast Adaptation in Dynamic POMDP Problems by Ben-Iwhiwhu, Eseoghene and Ladosz, Pawel and Dick, Jeffery and Chen, Wen-Hua and Pilly, Praveen and Soltoggio, Andrea http://arxiv.org/abs/2004.12846 Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning by Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey http://arxiv.org/abs/1910.10897 Meta reinforcement learning as task inference by Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A and Teh, Yee Whye and Heess, Nicolas http://arxiv.org/abs/1905.06424 Meta-Gradient Reinforcement Learning by Xu, Zhongwen and van Hasselt, Hado and Silver, David http://arxiv.org/abs/1805.09801 Self-Paced Deep Reinforcement Learning by Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni http://arxiv.org/abs/2004.11812 Scheduling the Learning Rate Via Hypergradients: New Insights and a New Algorithm by Donini, Michele and Franceschi, Luca and Majumder, Orchid and Pontil, Massimiliano and Frasconi, Paolo https://openreview.net/pdf?id=Ske6qJSKPH Learning Stabilizable Nonlinear Dynamics with Contraction-Based Regularization by Singh, Sumeet and Richards, Spencer M and Sindhwani, Vikas and Slotine, Jean-Jacques E and Pavone, Marco http://arxiv.org/abs/1907.13122 A Comprehensive Overview and Survey of Recent Advances in Meta-Learning by Peng, Huimin http://arxiv.org/abs/2004.11149 Learning a Formula of Interpretability to Learn Interpretable Formulas by Virgolin, Marco and De Lorenzo, Andrea and Medvet, Eric and Randone, Francesca http://arxiv.org/abs/2004.11170 Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads by Belkhale, Suneel and Li, Rachel and Kahn, Gregory and McAllister, Rowan and Calandra, Roberto and Levine, Sergey http://arxiv.org/abs/2004.11345 Frustratingly Simple Few-Shot Object Detection by Wang, Xin and Huang, Thomas E and Darrell, Trevor and Gonzalez, Joseph E and Yu, Fisher http://arxiv.org/abs/2003.06957 Meta Pseudo Labels by Pham, Hieu and Xie, Qizhe and Dai, Zihang and Le, Quoc V http://arxiv.org/abs/2003.10580 0e56da12-a2f0-4288-b745-c15deec9183a by Unknown http://learn2learn.net Finding online neural update rules by learning to remember by Gregor, Karol http://arxiv.org/abs/2003.03124 A New Meta-Baseline for Few-Shot Learning by Chen, Yinbo and Wang, Xiaolong and Liu, Zhuang and Xu, Huijuan and Darrell, Trevor http://arxiv.org/abs/2003.04390 Learning to be Global Optimizer by Zhang, Haotian and Sun, Jianyong and Xu, Zongben http://arxiv.org/abs/2003.04521 Scalable Multi-Task Imitation Learning with Autonomous Improvement by Singh, Avi and Jang, Eric and Irpan, Alexander and Kappler, Daniel and Dalal, Murtaza and Levine, Sergey and Khansari, Mohi and Finn, Chelsea http://arxiv.org/abs/2003.02636 Meta-learning for mixed linear regression by Kong, Weihao and Somani, Raghav and Song, Zhao and Kakade, Sham and Oh, Sewoong http://arxiv.org/abs/2002.08936 Provable Meta-Learning of Linear Representations by Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael I http://arxiv.org/abs/2002.11684 Learning to Continually Learn by Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff and Cheney, Nick http://arxiv.org/abs/2002.09571 PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees by Rothfuss, Jonas and Fortuin, Vincent and Krause, Andreas http://arxiv.org/abs/2002.05551 Incremental Learning for Metric-Based Meta-Learners by Liu, Qing and Majumder, Orchid and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano http://arxiv.org/abs/2002.04162 Hyper-Meta Reinforcement Learning with Sparse Reward by Hua, Yun and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Yan, Junchi and He, Xiaofeng and Zha, Hongyuan http://arxiv.org/abs/2002.04238 Meta-Learning across Meta-Tasks for Few-Shot Learning by Fei, Nanyi and Lu, Zhiwu and Gao, Yizhao and Tian, Jia and Xiang, Tao and Wen, Ji-Rong http://arxiv.org/abs/2002.04274 Distribution-Agnostic Model-Agnostic Meta-Learning by Collins, Liam and Mokhtari, Aryan and Shakkottai, Sanjay http://arxiv.org/abs/2002.04766 Provably Convergent Policy Gradient Methods for Model-Agnostic Meta-Reinforcement Learning by Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman http://arxiv.org/abs/2002.05135 Meta-learning framework with applications to zero-shot time-series forecasting by Oreshkin, Boris N and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua http://arxiv.org/abs/2002.02887 A Loss-Function for Causal Machine-Learning by Yang, I-Sheng http://arxiv.org/abs/2001.00629 Self-Tuning Deep Reinforcement Learning by Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Van Hasslet, Hado and Silver, David and Singh, Satinder http://arxiv.org/abs/2002.12928 Learning Adaptive Loss for Robust Learning with Noisy Labels by Shu, Jun and Zhao, Qian and Chen, Keyu and Xu, Zongben and Meng, Deyu http://arxiv.org/abs/2002.06482 A Structured Prediction Approach for Conditional Meta-Learning by Wang, Ruohan and Demiris, Yiannis and Ciliberto, Carlo http://arxiv.org/abs/2002.08799 Curriculum in Gradient-Based Meta-Reinforcement Learning by Mehta, Bhairav and Deleu, Tristan and Raparthy, Sharath Chandra and Pal, Chris J and Paull, Liam http://arxiv.org/abs/2002.07956 Multi-Step Model-Agnostic Meta-Learning: Convergence and Improved Algorithms by Ji, Kaiyi and Yang, Junjie and Liang, Yingbin http://arxiv.org/abs/2002.07836 Local Nonparametric Meta-Learning by Goo, Wonjoon and Niekum, Scott http://arxiv.org/abs/2002.03272 Revisiting Meta-Learning as Supervised Learning by Chao, Wei-Lun and Ye, Han-Jia and Zhan, De-Chuan and Campbell, Mark and Weinberger, Kilian Q http://arxiv.org/abs/2002.00573 SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning by Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and van der Maaten, Laurens http://arxiv.org/abs/1911.04623 Fast and Generalized Adaptation for Few-Shot Learning by Song, Liang and Liu, Jinlu and Qin, Yongqiang http://arxiv.org/abs/1911.10807 Meta-Learning without Memorization by Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1912.03820 Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One by Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\\\"o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin http://arxiv.org/abs/1912.03263 MAME : Model-Agnostic Meta-Exploration by Gurumurthy, Swaminathan and Kumar, Sumit and Sycara, Katia http://arxiv.org/abs/1911.04024 Constructing Multiple Tasks for Augmentation: Improving Neural Image Classification With K-means Features by Gui, Tao and Qing, Lizhi and Zhang, Qi and Ye, Jiacheng and Yan, Hang and Fei, Zichu and Huang, Xuanjing http://arxiv.org/abs/1911.07518 Meta Adaptation using Importance Weighted Demonstrations by Lekkala, Kiran and Abu-El-Haija, Sami and Itti, Laurent http://arxiv.org/abs/1911.10322 VIABLE: Fast Adaptation via Backpropagating Learned Loss by Feng, Leo and Zintgraf, Luisa and Peng, Bei and Whiteson, Shimon http://arxiv.org/abs/1911.13159 Decoupling Adaptation from Modeling with Meta-Optimizers for Meta Learning by Arnold, S{\\'e}bastien M R and Iqbal, Shariq and Sha, Fei http://arxiv.org/abs/1910.13603 TADAM: Task dependent adaptive metric for improved few-shot learning by Oreshkin, Boris and Rodr{\\'\\i}guez L{\\'o}pez, Pau and Lacoste, Alexandre http://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning.pdf Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks by Bansal, Trapit and Jha, Rishikesh and McCallum, Andrew http://arxiv.org/abs/1911.03863 Optimizing Millions of Hyperparameters by Implicit Differentiation by Lorraine, Jonathan and Vicol, Paul and Duvenaud, David http://arxiv.org/abs/1911.02590 Meta-data: Characterization of Input Features for Meta-learning by Castiello, Ciro and Castellano, Giovanna and Fanelli, Anna Maria http://dx.doi.org/10.1007/11526018_45 Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems by Mi, Fei and Huang, Minlie and Zhang, Jiyong and Faltings, Boi http://arxiv.org/abs/1905.05644 Domain Generalization via Model-Agnostic Learning of Semantic Features by Dou, Qi and Castro, Daniel C and Kamnitsas, Konstantinos and Glocker, Ben http://arxiv.org/abs/1910.13580 Hierarchical Expert Networks for Meta-Learning by Hihn, Heinke and Braun, Daniel A http://arxiv.org/abs/1911.00348 Online Meta-Learning on Non-convex Setting by Zhuang, Zhenxun and Wang, Yunlong and Yu, Kezi and Lu, Songtao http://arxiv.org/abs/1910.10196 Learning-to-Learn Stochastic Gradient Descent with Biased Regularization by Denevi, Giulia and Ciliberto, Carlo and Grazzi, Riccardo and Pontil, Massimiliano http://arxiv.org/abs/1903.10399 Provable Guarantees for Gradient-Based Meta-Learning by Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet http://arxiv.org/abs/1902.10644 The TCGA Meta-Dataset Clinical Benchmark by Samiei, Mandana and W{\\\"u}rfl, Tobias and Deleu, Tristan and Weiss, Martin and Dutil, Francis and Fevens, Thomas and Boucher, Genevi{`e}ve and Lemieux, Sebastien and Cohen, Joseph Paul http://arxiv.org/abs/1910.08636 VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning by Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon http://arxiv.org/abs/1910.08348 Meta-Transfer Learning through Hard Tasks by Sun, Qianru and Liu, Yaoyao and Chen, Zhaozheng and Chua, Tat-Seng and Schiele, Bernt http://arxiv.org/abs/1910.03648 Model-Agnostic Meta-Learning using Runge-Kutta Methods by Im, Daniel Jiwoong and Jiang, Yibo and Verma, Nakul http://arxiv.org/abs/1910.07368 Improving Generalization in Meta Reinforcement Learning using Learned Objectives by Kirsch, Louis and van Steenkiste, Sjoerd and Schmidhuber, J{\\\"u}rgen http://arxiv.org/abs/1910.04098 Generalized Inner Loop Meta-Learning by Grefenstette, Edward and Amos, Brandon and Yarats, Denis and Htut, Phu Mon and Molchanov, Artem and Meier, Franziska and Kiela, Douwe and Cho, Kyunghyun and Chintala, Soumith http://arxiv.org/abs/1910.01727 Is Fast Adaptation All You Need? by Javed, Khurram and Yao, Hengshuai and White, Martha http://arxiv.org/abs/1910.01705 Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in Damaged Robots by Verma, Shresth and Nair, Haritha S and Agarwal, Gaurav and Dhar, Joydip and Shukla, Anupam http://arxiv.org/abs/1910.01240 ES-MAML: Simple Hessian-Free Meta Learning by Song, Xingyou and Gao, Wenbo and Yang, Yuxiang and Choromanski, Krzysztof and Pacchiano, Aldo and Tang, Yunhao http://arxiv.org/abs/1910.01215 Meta-Q-Learning by Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J http://arxiv.org/abs/1910.00125 Efficient meta reinforcement learning via meta goal generation by Fu, Haotian and Tang, Hongyao and Hao, Jianye http://arxiv.org/abs/1909.13607 Chameleon: Learning Model Initializations Across Tasks With Different Schemas by Brinkmeyer, Lukas and Drumond, Rafael Rego and Scholz, Randolf and Grabocka, Josif and Schmidt-Thieme, Lars http://arxiv.org/abs/1909.13576 Learning Fast Adaptation with Meta Strategy Optimization by Yu, Wenhao and Tan, Jie and Bai, Yunfei and Coumans, Erwin and Ha, Sehoon http://arxiv.org/abs/1909.12995 Meta-Inverse Reinforcement Learning with Probabilistic Context Variables by Yu, Lantao and Yu, Tianhe and Finn, Chelsea and Ermon, Stefano http://arxiv.org/abs/1909.09314 Modular Meta-Learning with Shrinkage by Chen, Yutian and Friesen, Abram L and Behbahani, Feryal and Budden, David and Hoffman, Matthew W and Doucet, Arnaud and de Freitas, Nando http://arxiv.org/abs/1909.05557 Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning by Farquhar, Gregory and Whiteson, Shimon and Foerster, Jakob http://arxiv.org/abs/1909.10549 Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML by Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol http://arxiv.org/abs/1909.09157 Meta-Learning by Vanschoren, Joaquin https://doi.org/10.1007/978-3-030-05318-5_2 Understanding Short-Horizon Bias in Stochastic Meta-Optimization by Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger http://arxiv.org/abs/1803.02021 On First-Order Meta-Learning Algorithms by Nichol, Alex and Achiam, Joshua and Schulman, John http://arxiv.org/abs/1803.02999 Towards Understanding Generalization in Gradient-Based Meta-Learning by Guiroy, Simon and Verma, Vikas and Pal, Christopher http://arxiv.org/abs/1907.07287 They empirically study the landscape of fast-adaptation in MAML. The most interesting claim is that when meta-overfitting, the loss landscape becomes flatter on test tasks. On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms by Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman http://arxiv.org/abs/1908.10400 Learning to Learn with Gradients by Finn, Chelsea http://learn2learn.net Acetylcholine and memory by Hasselmo, M E and Bower, J M https://www.ncbi.nlm.nih.gov/pubmed/7688162 A THEORY OF META-LEARNING AND PRINCIPLES OF FACILITATION: AN ORGANISMIC PERSPECTIVE by Maudsley, Donald B https://uosc.primo.exlibrisgroup.com/discovery/fulldisplay?docid=proquest302999651&context=PC&vid=01USC_INST:01USC&lang=en&search_scope=MyInst_and_CI&adaptor=Primo%20Central&tab=Everything&mode=Basic THE ROLE OF METALEARNING IN STUDY PROCESSES by Biggs, J B http://doi.wiley.com/10.1111/j.2044-8279.1985.tb02625.x Understanding and correcting pathologies in the training of learned optimizers by Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Daniel Freeman, C and Sohl-Dickstein, Jascha http://arxiv.org/abs/1810.10180 Provides many tricks (e.g. split train batch for model \\& opt, average gradient estimators) for training differentiable optimizers online. They also have a couple of interesting observations specific to recurrent optimizers. Learned Optimizers that Scale and Generalize by Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha http://arxiv.org/abs/1703.04813 Using learned optimizers to make models robust to input noise by Metz, Luke and Maheswaranathan, Niru and Shlens, Jonathon and Sohl-Dickstein, Jascha and Cubuk, Ekin D http://arxiv.org/abs/1906.03367 Learning to Optimize Neural Nets by Li, Ke and Malik, Jitendra http://arxiv.org/abs/1703.00441 Meta-Learning Update Rules for Unsupervised Representation Learning by Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and Sohl-Dickstein, Jascha http://arxiv.org/abs/1804.00222 Learning to Optimize by Li, Ke and Malik, Jitendra http://arxiv.org/abs/1606.01885 Learning to learn by gradient descent by gradient descent by Andrychowicz, M and Denil, M and Gomez, S http://learn2learn.net Online Learning Rate Adaptation with Hypergradient Descent by Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank http://arxiv.org/abs/1703.04782 They adapt the learning rate of SGD by differentiating the loss of the next parameters w.r.t. the learning rate. They observe that the gradient of the learning rate is simply the inner product of the last two gradients. Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta by Sutton, Richard S http://dx.doi.org/ What's mostly interesting in this paper is the adaptation of delta-bar-delta to the online scenario. The idea of representing the learning rate as an exponential is nice. Also nice to see that the derivation suggests a full-matrix adaptive case. Gain adaptation beats least squares by Sutton, Richard S https://pdfs.semanticscholar.org/7ec8/876f219b3b3d5c894a3f395c89c382029cc5.pdf This paper extends IDBD as algorithms K1 and K2, but from my quick read, it isn't clear what's the motivation for those modifications. (Seems to work in a `normalized space'', {\\ a} la natural gradient ?)They do work better. Local Gain Adaptation in Stochastic Gradient Descent by Schraudolph, Nicol N https://pdfs.semanticscholar.org/31a0/b86c3cd04e6539626f34b80db7ff79d23f40.pdf This algorithm extends IDBD (Sutton) to the non-linear setting. Interestingly, they have a few brief discussionson the difficulties to optimize at the meta-level. (c.f. Meta-level conditioning section.) Overall, it shines light on the ground idea behind IDBD. TIDBD: Adapting Temporal-difference Step-sizes Through Stochastic Meta-descent by Kearney, Alex and Veeriah, Vivek and Travnik, Jaden B and Sutton, Richard S and Pilarski, Patrick M http://arxiv.org/abs/1804.03334 Increased rates of convergence through learning rate adaptation by Jacobs, Robert A http://www.sciencedirect.com/science/article/pii/0893608088900032 This paper argues that we need (at least) four ingredients to improve optimization of connectionist networks: 1. each parameter has its own stepsize, 2. stepsizes vary over time, 3. if consecutive gradients of a stepsize have the same sign, the stepsize should be increased, 4. conversely, if the stepsize should be decreased if its gradients have opposite signs. It also proposes to use two improvements: 1. Momentum (i.e. Polyak's heavyball), 2. delta-bar-delta (i.e. learning the stepsize). It has an interesting comment on the difficulty of learning the stepsize, and therefore comes up with a ``hack'' that outperforms momentum. Meta-descent for Online, Continual Prediction by Jacobsen, Andrew and Schlegel, Matthew and Linke, Cameron and Degris, Thomas and White, Adam and White, Martha http://arxiv.org/abs/1907.07751 The idea is to learn the learning rate so as to minimize the norm of the gradient. They argue that for the continual learning setting, this forces the algorithm to stay ``as stable as possible''. No theorems, small-scale (but interesting) experiments. Adaptation of learning rate parameters by Sutton, Rich http://learn2learn.net Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace by Lee, Yoonho and Choi, Seungjin http://arxiv.org/abs/1801.05558 Meta-Learning with Warped Gradient Descent by Flennerhag, Sebastian and Rusu, Andrei A and Pascanu, Razvan and Yin, Hujun and Hadsell, Raia http://arxiv.org/abs/1909.00025 Meta-Learning via Learned Loss by Chebotar, Yevgen and Molchanov, Artem and Bechtle, Sarah and Righetti, Ludovic and Meier, Franziska and Sukhatme, Gaurav http://arxiv.org/abs/1906.05374 They learn the loss as a NN, and that loss's objective is to maximize the sum of rewards. It is provided a bunch of things, including inputs, outputs, goals. Meta-Curvature by Park, Eunbyung and Oliva, Junier B http://arxiv.org/abs/1902.03356 Alpha MAML: Adaptive Model-Agnostic Meta-Learning by Behl, Harkirat Singh and Baydin, At{\\i}l{\\i}m G{\\\"u}ne{\\c s} and Torr, Philip H S http://arxiv.org/abs/1905.07435 They combine hypergradient and MAML: adapt all learning rates at all times. Meta-SGD: Learning to Learn Quickly for Few-Shot Learning by Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang http://arxiv.org/abs/1707.09835 ProMP: Proximal Meta-Policy Search by Rothfuss, Jonas and Lee, Dennis and Clavera, Ignasi and Asfour, Tamim and Abbeel, Pieter http://arxiv.org/abs/1810.06784 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks by Finn, Chelsea and Abbeel, Pieter and Levine, Sergey http://learn2learn.net Optimization as a model for few-shot learning by Ravi, Sachin and Larochelle, Hugo https://openreview.net/pdf?id=rJY0-Kcll Fast Context Adaptation via Meta-Learning by Zintgraf, Luisa M and Shiarlis, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon http://arxiv.org/abs/1810.03642 Meta-Learning with Implicit Gradients by Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham and Levine, Sergey http://arxiv.org/abs/1909.04630 Natural Neural Networks by Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray http://dl.acm.org/citation.cfm?id=2969442.2969471 A Baseline for Few-Shot Image Classification by Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano http://arxiv.org/abs/1909.02729 A CLOSER LOOK AT FEW-SHOT CLASSIFICATION by Chen, Wei-Yu and Liu, Yen-Cheng and Kira, Zsolt https://openreview.net/pdf?id=HkxLXnAcFQ Suggests that meta-learning papers haven't been tested against classical baselines. When considering those baselines, they perform better than many of the recent meta-learning techniques. Meta-learning with differentiable closed-form solvers by Bertinetto, Luca and Henriques, Joao F and Torr, Philip and Vedaldi, Andrea https://openreview.net/forum?id=HyxnZh0ct7 Uncertainty in Model-Agnostic Meta-Learning using Variational Inference by Nguyen, Cuong and Do, Thanh-Toan and Carneiro, Gustavo http://arxiv.org/abs/1907.11864 Meta-Reinforcement Learning of Structured Exploration Strategies by Gupta, Abhishek and Mendonca, Russell and Liu, Yuxuan and Abbeel, Pieter and Levine, Sergey http://arxiv.org/abs/1802.07245 Metalearned Neural Memory by Munkhdalai, Tsendsuren and Sordoni, Alessandro and Wang, Tong and Trischler, Adam http://arxiv.org/abs/1907.09720 Accelerated Stochastic Approximation by Kesten, Harry https://projecteuclid.org/euclid.aoms/1177706705 Meta-Learning for Black-box Optimization by Vishnu, T V and Malhotra, Pankaj and Narwariya, Jyoti and Vig, Lovekesh and Shroff, Gautam http://arxiv.org/abs/1907.06901 They essentially extend the recurrent meta-learning framework in a few ways: 1. Use regret instead of objective improvement as meta-learning objective. 2. Normalize the objective so as to make it play nice with LSTMs. 3. Incorporate domain-constraints, so that the LSTM always outputs feasible solutions. All are described in page 3. Task Agnostic Continual Learning via Meta Learning by He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan http://arxiv.org/abs/1906.05201 Watch, Try, Learn: Meta-Learning from Demonstrations and Reward by Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1906.03352 Meta-Learning Representations for Continual Learning by Javed, Khurram and White, Martha http://arxiv.org/abs/1905.12588 TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning by Yoon, Sung Whan and Seo, Jun and Moon, Jaekyun http://arxiv.org/abs/1905.06549 Meta Reinforcement Learning with Task Embedding and Shared Policy by Lan, Lin and Li, Zhenguo and Guan, Xiaohong and Wang, Pinghui http://arxiv.org/abs/1905.06527 Hierarchically Structured Meta-learning by Yao, Huaxiu and Wei, Ying and Huang, Junzhou and Li, Zhenhui http://arxiv.org/abs/1905.05301 Curious Meta-Controller: Adaptive Alternation between Model-Based and Model-Free Control in Deep Reinforcement Learning by Hafez, Muhammad Burhan and Weber, Cornelius and Kerzel, Matthias and Wermter, Stefan http://arxiv.org/abs/1905.01718 Learning to Learn in Simulation by Teng, Ervin and Iannucci, Bob http://arxiv.org/abs/1902.01569 Meta-Learning with Differentiable Convex Optimization by Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano http://arxiv.org/abs/1904.03758 Functional Regularisation for Continual Learning by Titsias, Michalis K and Schwarz, Jonathan and de G. Matthews, Alexander G and Pascanu, Razvan and Teh, Yee Whye http://arxiv.org/abs/1901.11356 Learning to Forget for Meta-Learning by Baik, Sungyong and Hong, Seokil and Lee, Kyoung Mu http://arxiv.org/abs/1906.05895 Meta-learning of Sequential Strategies by Ortega, Pedro A and Wang, Jane X and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and Jayakumar, Siddhant M and McGrath, Tom and Miller, Kevin and Azar, Mohammad and Osband, Ian and Rabinowitz, Neil and Gy{\\\"o}rgy, Andr{\\'a}s and Chiappa, Silvia and Osindero, Simon and Teh, Yee Whye and van Hasselt, Hado and de Freitas, Nando and Botvinick, Matthew and Legg, Shane http://arxiv.org/abs/1905.03030 This paper essentially provides a theoretical framework to ground the fact that recurrent meta-learning (RL^2, LLGD^2) performs Bayesian inference during adaptation. Auto-Meta: Automated Gradient Based Meta Learner Search by Kim, Jaehong and Lee, Sangyeul and Kim, Sungwan and Cha, Moonsu and Lee, Jung Kwon and Choi, Youngduck and Choi, Yongseok and Cho, Dong-Yeon and Kim, Jiwon http://arxiv.org/abs/1806.06927 Adaptive Gradient-Based Meta-Learning Methods by Khodak, Mikhail and Florina-Balcan, Maria and Talwalkar, Ameet http://arxiv.org/abs/1906.02717 Embedded Meta-Learning: Toward more flexible deep-learning models by Lampinen, Andrew K and McClelland, James L http://arxiv.org/abs/1905.09950 Modular meta-learning by Alet, Ferran and Lozano-P{\\'e}rez, Tom{\\'a}s and Kaelbling, Leslie P http://arxiv.org/abs/1806.10166 MetaPred: Meta-Learning for Clinical Risk Prediction with Limited Patient Electronic Health Records by Zhang, Xi Sheryl and Tang, Fengyi and Dodge, Hiroko and Zhou, Jiayu and Wang, Fei http://arxiv.org/abs/1905.03218 Prototypical Networks for Few-shot Learning by Snell, Jake and Swersky, Kevin and Zemel, Richard S http://arxiv.org/abs/1703.05175 Meta-learners' learning dynamics are unlike learners' by Rabinowitz, Neil C http://arxiv.org/abs/1905.01320 Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity by Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O https://openreview.net/forum?id=r1lrAiA5Ym Reinforcement Learning, Fast and Slow by Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis http://dx.doi.org/10.1016/j.tics.2019.02.006 Been There, Done That: Meta-Learning with Episodic Recall by Ritter, Samuel and Wang, Jane X and Kurth-Nelson, Zeb and Jayakumar, Siddhant M and Blundell, Charles and Pascanu, Razvan and Botvinick, Matthew http://arxiv.org/abs/1805.09692 Guided Meta-Policy Search by Mendonca, Russell and Gupta, Abhishek and Kralev, Rosen and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1904.00956 Hierarchical Meta Learning by Zou, Yingtian and Feng, Jiashi http://arxiv.org/abs/1904.09081 A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms by Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher http://arxiv.org/abs/1901.10912 Generalize Across Tasks: Efficient Algorithms for Linear Representation Learning by Bullins, Brian and Hazan, Elad and Kalai, Adam and Livni, Roi http://proceedings.mlr.press/v98/bullins19a.html Incremental Learning-to-Learn with Statistical Guarantees by Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano http://arxiv.org/abs/1803.08089 A Model of Inductive Bias Learning by Baxter, J http://arxiv.org/abs/1106.0245 Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables by Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey http://arxiv.org/abs/1903.08254 Continual Learning with Tiny Episodic Memories by Chaudhry, Arslan and Rohrbach, Marcus and Elhoseiny, Mohamed and Ajanthan, Thalaiyasingam and Dokania, Puneet K and Torr, Philip H S and Ranzato, Marc'aurelio http://arxiv.org/abs/1902.10486 Online Meta-Learning by Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey http://arxiv.org/abs/1902.08438 Modulating transfer between tasks in gradient-based meta-learning by Grant, Erin and Jerfel, Ghassen and Heller, Katherine and Griffiths, Thomas L https://openreview.net/pdf?id=HyxpNnRcFX Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning by Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea http://arxiv.org/abs/1803.11347 Meta-Learning with Latent Embedding Optimization by Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia http://arxiv.org/abs/1807.05960 Learning to Generalize: Meta-Learning for Domain Generalization by Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M http://arxiv.org/abs/1710.03463 Some Considerations on Learning to Explore via Meta-Reinforcement Learning by Stadie, Bradly C and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya http://arxiv.org/abs/1803.01118 How to train your MAML by Antoniou, Antreas and Edwards, Harrison and Storkey, Amos http://arxiv.org/abs/1810.09502 Bayesian Model-Agnostic Meta-Learning by Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin http://arxiv.org/abs/1806.03836 Probabilistic Model-Agnostic Meta-Learning by Finn, Chelsea and Xu, Kelvin and Levine, Sergey http://arxiv.org/abs/1806.02817 The effects of negative adaptation in Model-Agnostic Meta-Learning by Deleu, Tristan and Bengio, Yoshua http://arxiv.org/abs/1812.02159 Memory-based Parameter Adaptation by Sprechmann, Pablo and Jayakumar, Siddhant M and Rae, Jack W and Pritzel, Alexander and Badia, Adri{`a} Puigdom{`e}nech and Uria, Benigno and Vinyals, Oriol and Hassabis, Demis and Pascanu, Razvan and Blundell, Charles http://arxiv.org/abs/1802.10542 Deep Meta-Learning: Learning to Learn in the Concept Space by Zhou, Fengwei and Wu, Bin and Li, Zhenguo http://arxiv.org/abs/1802.03596 Deep Prior by Lacoste, Alexandre and Boquet, Thomas and Rostamzadeh, Negar and Oreshkin, Boris and Chung, Wonchang and Krueger, David http://arxiv.org/abs/1712.05016 Recasting Gradient-Based Meta-Learning as Hierarchical Bayes by Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas http://arxiv.org/abs/1801.08930 WNGrad: Learn the Learning Rate in Gradient Descent by Wu, Xiaoxia and Ward, Rachel and Bottou, L{\\'e}on http://arxiv.org/abs/1803.02865 Learning to Learn by Finn, Chelsea http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/ Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments by Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter http://arxiv.org/abs/1710.03641","title":"Submitted Papers"},{"location":"paper_list/#submission-form","text":"Loading\u2026","title":"Submission Form"},{"location":"docs/learn2learn.algorithms/","text":"learn2learn.algorithms \u00b6 A set of high-level algorithm implementations, with easy-to-use API. MAML \u00b6 1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt() methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () adapt \u00b6 1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. clone \u00b6 1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. MetaSGD \u00b6 1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () clone \u00b6 1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates. adapt \u00b6 1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates. GBML \u00b6 1 2 3 4 5 6 7 8 GBML ( module , transform , lr = 1.0 , adapt_transform = False , first_order = False , allow_unused = False , allow_nograd = False , ** kwargs ) [Source] Description General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5]. Arguments module (Module) - Module to be wrapped. tranform (Module) - Transform used to update the module. lr (float) - Fast adaptation learning rate. adapt_transform (bool, optional , default=False) - Whether to update the transform's parameters during fast-adaptation. first_order (bool, optional , default=False) - Whether to use the first-order approximation. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d Park & Oliva. 2019. \u201cMeta-Curvature.\u201d Behl et al. 2019. \u201cAlpha MAML: Adaptive Model-Agnostic Meta-Learning.\u201d Arnold et al. 2019. \u201cWhen MAML Can Adapt Fast and How to Assist When It Cannot.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 model = SmallCNN () transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) gbml = l2l . algorithms . GBML ( module = model , transform = transform , lr = 0.01 , adapt_transform = True , ) gbml . to ( device ) opt = torch . optim . SGD ( gbml . parameters (), lr = 0.001 ) __Training with 1 adaptation step__ for iteration in range ( 10 ): opt . zero_grad () task_model = gbml . clone () loss = compute_loss ( task_model ) task_model . adapt ( loss ) loss . backward () opt . step () clone \u00b6 1 2 3 4 GBML . clone ( first_order = None , allow_unused = None , allow_nograd = None , adapt_transform = None ) Description Similar to MAML.clone() . Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. adapt \u00b6 1 GBML . adapt ( loss , first_order = None , allow_nograd = None , allow_unused = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True . Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#learn2learnalgorithms","text":"A set of high-level algorithm implementations, with easy-to-use API.","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#maml","text":"1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt() methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MAML"},{"location":"docs/learn2learn.algorithms/#adapt","text":"1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#clone","text":"1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"clone"},{"location":"docs/learn2learn.algorithms/#metasgd","text":"1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MetaSGD"},{"location":"docs/learn2learn.algorithms/#clone_1","text":"1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_1","text":"1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#gbml","text":"1 2 3 4 5 6 7 8 GBML ( module , transform , lr = 1.0 , adapt_transform = False , first_order = False , allow_unused = False , allow_nograd = False , ** kwargs ) [Source] Description General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5]. Arguments module (Module) - Module to be wrapped. tranform (Module) - Transform used to update the module. lr (float) - Fast adaptation learning rate. adapt_transform (bool, optional , default=False) - Whether to update the transform's parameters during fast-adaptation. first_order (bool, optional , default=False) - Whether to use the first-order approximation. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d Park & Oliva. 2019. \u201cMeta-Curvature.\u201d Behl et al. 2019. \u201cAlpha MAML: Adaptive Model-Agnostic Meta-Learning.\u201d Arnold et al. 2019. \u201cWhen MAML Can Adapt Fast and How to Assist When It Cannot.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 model = SmallCNN () transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) gbml = l2l . algorithms . GBML ( module = model , transform = transform , lr = 0.01 , adapt_transform = True , ) gbml . to ( device ) opt = torch . optim . SGD ( gbml . parameters (), lr = 0.001 ) __Training with 1 adaptation step__ for iteration in range ( 10 ): opt . zero_grad () task_model = gbml . clone () loss = compute_loss ( task_model ) task_model . adapt ( loss ) loss . backward () opt . step ()","title":"GBML"},{"location":"docs/learn2learn.algorithms/#clone_2","text":"1 2 3 4 GBML . clone ( first_order = None , allow_unused = None , allow_nograd = None , adapt_transform = None ) Description Similar to MAML.clone() . Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_2","text":"1 GBML . adapt ( loss , first_order = None , allow_nograd = None , allow_unused = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True . Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"adapt"},{"location":"docs/learn2learn.data/","text":"learn2learn.data \u00b6 A set of utilities for data & tasks loading, preprocessing, and sampling. MetaDataset \u00b6 1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist ) TaskDataset \u00b6 1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task learn2learn.data.transforms \u00b6 Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function. LoadData \u00b6 1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample. NWays \u00b6 1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. KShots \u00b6 1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement. FilterLabels \u00b6 1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include. FusedNWaysKShots \u00b6 1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset. RemapLabels \u00b6 1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample. ConsecutiveLabels \u00b6 1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#learn2learndata","text":"A set of utilities for data & tasks loading, preprocessing, and sampling.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#metadataset","text":"1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist )","title":"MetaDataset"},{"location":"docs/learn2learn.data/#taskdataset","text":"1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task","title":"TaskDataset"},{"location":"docs/learn2learn.data/#learn2learndatatransforms","text":"Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function.","title":"learn2learn.data.transforms"},{"location":"docs/learn2learn.data/#loaddata","text":"1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"LoadData"},{"location":"docs/learn2learn.data/#nways","text":"1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels.","title":"NWays"},{"location":"docs/learn2learn.data/#kshots","text":"1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement.","title":"KShots"},{"location":"docs/learn2learn.data/#filterlabels","text":"1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include.","title":"FilterLabels"},{"location":"docs/learn2learn.data/#fusednwayskshots","text":"1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset.","title":"FusedNWaysKShots"},{"location":"docs/learn2learn.data/#remaplabels","text":"1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"RemapLabels"},{"location":"docs/learn2learn.data/#consecutivelabels","text":"1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"ConsecutiveLabels"},{"location":"docs/learn2learn.gym/","text":"learn2learn.gym \u00b6 Environment, models, and other utilities related to reinforcement learning and OpenAI Gym. MetaEnv \u00b6 1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations. AsyncVectorEnv \u00b6 1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations. learn2learn.gym.envs.mujoco \u00b6 HalfCheetahForwardBackwardEnv \u00b6 1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntForwardBackwardEnv \u00b6 1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntDirectionEnv \u00b6 1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidForwardBackwardEnv \u00b6 1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidDirectionEnv \u00b6 1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. learn2learn.gym.envs.particles \u00b6 Particles2DEnv \u00b6 1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation. learn2learn.gym.envs.metaworld \u00b6 MetaWorldML1 \u00b6 1 MetaWorldML1 ( task_name , env_type = 'train' , n_goals = 50 , sample_all = False ) [Source] Description The ML1 Benchmark of Meta-World is focused on solving just one task on different object / goal configurations.This task can be either one of the following: 'reach', 'push' and 'pick-and-place'. The meta-training is performed on a set of 50 randomly chosen once initial object and goal positions. The meta-testing is performed on a held-out set of 10 new different configurations. The starting state of the robot arm is always fixed. The goal positions are not provided in the observation space, forcing the Sawyer robot arm to explore and adapt to the new goal through trial-and-error. This is considered a relatively easy problem for a meta-learning algorithm to solve and acts as a sanity check to a working implementation. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019). MetaWorldML10 \u00b6 1 MetaWorldML10 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description The ML10 Benchmark of Meta-World consists of 10 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. The meta-training tasks have been intentionally selected to have a structural similarity to the test tasks. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This is a much harder problem than ML1 which probably requires more samples to train. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019). MetaWorldML45 \u00b6 1 MetaWorldML45 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description Similarly to ML10, this Benchmark has a variety of 45 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This benchmark is significantly difficult to solve due to the diversity across tasks. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#learn2learngym","text":"Environment, models, and other utilities related to reinforcement learning and OpenAI Gym.","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#metaenv","text":"1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations.","title":"MetaEnv"},{"location":"docs/learn2learn.gym/#asyncvectorenv","text":"1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations.","title":"AsyncVectorEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmujoco","text":"","title":"learn2learn.gym.envs.mujoco"},{"location":"docs/learn2learn.gym/#halfcheetahforwardbackwardenv","text":"1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HalfCheetahForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antforwardbackwardenv","text":"1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antdirectionenv","text":"1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntDirectionEnv"},{"location":"docs/learn2learn.gym/#humanoidforwardbackwardenv","text":"1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#humanoiddirectionenv","text":"1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidDirectionEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsparticles","text":"","title":"learn2learn.gym.envs.particles"},{"location":"docs/learn2learn.gym/#particles2denv","text":"1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"Particles2DEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmetaworld","text":"","title":"learn2learn.gym.envs.metaworld"},{"location":"docs/learn2learn.gym/#metaworldml1","text":"1 MetaWorldML1 ( task_name , env_type = 'train' , n_goals = 50 , sample_all = False ) [Source] Description The ML1 Benchmark of Meta-World is focused on solving just one task on different object / goal configurations.This task can be either one of the following: 'reach', 'push' and 'pick-and-place'. The meta-training is performed on a set of 50 randomly chosen once initial object and goal positions. The meta-testing is performed on a held-out set of 10 new different configurations. The starting state of the robot arm is always fixed. The goal positions are not provided in the observation space, forcing the Sawyer robot arm to explore and adapt to the new goal through trial-and-error. This is considered a relatively easy problem for a meta-learning algorithm to solve and acts as a sanity check to a working implementation. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML1"},{"location":"docs/learn2learn.gym/#metaworldml10","text":"1 MetaWorldML10 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description The ML10 Benchmark of Meta-World consists of 10 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. The meta-training tasks have been intentionally selected to have a structural similarity to the test tasks. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This is a much harder problem than ML1 which probably requires more samples to train. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML10"},{"location":"docs/learn2learn.gym/#metaworldml45","text":"1 MetaWorldML45 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description Similarly to ML10, this Benchmark has a variety of 45 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This benchmark is significantly difficult to solve due to the diversity across tasks. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML45"},{"location":"docs/learn2learn/","text":"learn2learn \u00b6 clone_module \u00b6 1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net. detach_module \u00b6 1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net. update_module \u00b6 1 update_module ( module , updates = None ) [Source] Description Updates the parameters of a module in-place, in a way that preserves differentiability. The parameters of the module are swapped with their update values, according to: p \\gets p + u, where p is the parameter, and u is its corresponding update. Arguments module (Module) - The module to update. updates (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the tensors in .update attributes. Example 1 2 3 4 5 6 7 8 error = loss ( model ( X ), y ) grads = torch . autograd . grad ( error , model . parameters (), create_graph = True , ) updates = [ - lr * g for g in grads ] l2l . update_module ( model , updates = updates ) magic_box \u00b6 1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"learn2learn"},{"location":"docs/learn2learn/#learn2learn","text":"","title":"learn2learn"},{"location":"docs/learn2learn/#clone_module","text":"1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net.","title":"clone_module"},{"location":"docs/learn2learn/#detach_module","text":"1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net.","title":"detach_module"},{"location":"docs/learn2learn/#update_module","text":"1 update_module ( module , updates = None ) [Source] Description Updates the parameters of a module in-place, in a way that preserves differentiability. The parameters of the module are swapped with their update values, according to: p \\gets p + u, where p is the parameter, and u is its corresponding update. Arguments module (Module) - The module to update. updates (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the tensors in .update attributes. Example 1 2 3 4 5 6 7 8 error = loss ( model ( X ), y ) grads = torch . autograd . grad ( error , model . parameters (), create_graph = True , ) updates = [ - lr * g for g in grads ] l2l . update_module ( model , updates = updates )","title":"update_module"},{"location":"docs/learn2learn/#magic_box","text":"1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"magic_box"},{"location":"docs/learn2learn.nn/","text":"learn2learn.nn \u00b6 Additional torch.nn.Module s frequently used for meta-learning. Lambda \u00b6 1 Lambda ( lmb ) [Source] Description Utility class to create a wrapper based on a lambda function. Arguments lmb (callable) - The function to call in the forward pass. Example 1 2 3 4 mean23 = Lambda ( lambda x : x . mean ( dim = [ 2 , 3 ])) # mean23 is a Module x = features ( img ) x = mean23 ( x ) x = x . flatten () Flatten \u00b6 1 Flatten () [Source] Description Utility Module to flatten inputs to (batch_size, -1) shape. Example 1 2 3 4 flatten = Flatten () x = torch . randn ( 5 , 3 , 32 , 32 ) x = flatten ( x ) print ( x . shape ) # (5, 3072) Scale \u00b6 1 Scale ( shape , alpha = 1.0 ) [Source] Description A per-parameter scaling factor with learnable parameter. Arguments shape (int or tuple) - The shape of the scaling matrix. alpha (float, optional , default=1.0) - Initial value for the scaling factor. Example 1 2 3 x = torch . ones ( 3 ) scale = Scale ( x . shape , alpha = 0.5 ) print ( scale ( x )) # [.5, .5, .5] KroneckerLinear \u00b6 1 KroneckerLinear ( n , m , bias = True , psd = False , device = None ) [Source] Description A linear transformation whose parameters are expressed as a Kronecker product. This Module maps an input vector x \\in \\mathbb{R}^{nm} to y = Ax + b such that: A = R^\\top \\otimes L, where L \\in \\mathbb{R}^{n \\times n} and R \\in \\mathbb{R}^{m \\times m} are the learnable Kronecker factors. This implementation can reduce the memory requirement for large linear mapping from \\mathcal{O}(n^2 \\cdot m^2) to \\mathcal{O}(n^2 + m^2) , but forces y \\in \\mathbb{R}^{nm} . The matrix A is initialized as the identity, and the bias as a zero vector. Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. psd (bool, optional , default=False) - Forces the matrix A to be positive semi-definite if True. device (device, optional , default=None) - The device on which to instantiate the Module. References Jose et al. 2018. \"Kronecker recurrent units\". Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 m , n = 2 , 3 x = torch . randn ( 6 ) kronecker = KroneckerLinear ( n , m ) y = kronecker ( x ) y . shape # (6, ) KroneckerRNN \u00b6 1 KroneckerRNN ( n , m , bias = True , sigma = None ) [Source] Description Implements a recurrent neural network whose matrices are parameterized via their Kronecker factors. (See KroneckerLinear for details.) Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerRNN ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, ) KroneckerLSTM \u00b6 1 KroneckerLSTM ( n , m , bias = True , sigma = None ) [Source] Description Implements an LSTM using a factorization similar to the one of KroneckerLinear . Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerLSTM ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"learn2learn.nn"},{"location":"docs/learn2learn.nn/#learn2learnnn","text":"Additional torch.nn.Module s frequently used for meta-learning.","title":"learn2learn.nn"},{"location":"docs/learn2learn.nn/#lambda","text":"1 Lambda ( lmb ) [Source] Description Utility class to create a wrapper based on a lambda function. Arguments lmb (callable) - The function to call in the forward pass. Example 1 2 3 4 mean23 = Lambda ( lambda x : x . mean ( dim = [ 2 , 3 ])) # mean23 is a Module x = features ( img ) x = mean23 ( x ) x = x . flatten ()","title":"Lambda"},{"location":"docs/learn2learn.nn/#flatten","text":"1 Flatten () [Source] Description Utility Module to flatten inputs to (batch_size, -1) shape. Example 1 2 3 4 flatten = Flatten () x = torch . randn ( 5 , 3 , 32 , 32 ) x = flatten ( x ) print ( x . shape ) # (5, 3072)","title":"Flatten"},{"location":"docs/learn2learn.nn/#scale","text":"1 Scale ( shape , alpha = 1.0 ) [Source] Description A per-parameter scaling factor with learnable parameter. Arguments shape (int or tuple) - The shape of the scaling matrix. alpha (float, optional , default=1.0) - Initial value for the scaling factor. Example 1 2 3 x = torch . ones ( 3 ) scale = Scale ( x . shape , alpha = 0.5 ) print ( scale ( x )) # [.5, .5, .5]","title":"Scale"},{"location":"docs/learn2learn.nn/#kroneckerlinear","text":"1 KroneckerLinear ( n , m , bias = True , psd = False , device = None ) [Source] Description A linear transformation whose parameters are expressed as a Kronecker product. This Module maps an input vector x \\in \\mathbb{R}^{nm} to y = Ax + b such that: A = R^\\top \\otimes L, where L \\in \\mathbb{R}^{n \\times n} and R \\in \\mathbb{R}^{m \\times m} are the learnable Kronecker factors. This implementation can reduce the memory requirement for large linear mapping from \\mathcal{O}(n^2 \\cdot m^2) to \\mathcal{O}(n^2 + m^2) , but forces y \\in \\mathbb{R}^{nm} . The matrix A is initialized as the identity, and the bias as a zero vector. Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. psd (bool, optional , default=False) - Forces the matrix A to be positive semi-definite if True. device (device, optional , default=None) - The device on which to instantiate the Module. References Jose et al. 2018. \"Kronecker recurrent units\". Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 m , n = 2 , 3 x = torch . randn ( 6 ) kronecker = KroneckerLinear ( n , m ) y = kronecker ( x ) y . shape # (6, )","title":"KroneckerLinear"},{"location":"docs/learn2learn.nn/#kroneckerrnn","text":"1 KroneckerRNN ( n , m , bias = True , sigma = None ) [Source] Description Implements a recurrent neural network whose matrices are parameterized via their Kronecker factors. (See KroneckerLinear for details.) Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerRNN ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"KroneckerRNN"},{"location":"docs/learn2learn.nn/#kroneckerlstm","text":"1 KroneckerLSTM ( n , m , bias = True , sigma = None ) [Source] Description Implements an LSTM using a factorization similar to the one of KroneckerLinear . Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerLSTM ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"KroneckerLSTM"},{"location":"docs/learn2learn.optim/","text":"learn2learn.optim \u00b6 A set of utilities to write differentiable optimization algorithms. LearnableOptimizer \u00b6 1 LearnableOptimizer ( model , transform , lr = 1.0 ) [Source] Description A PyTorch Optimizer with learnable transform, enabling the implementation of meta-descent / hyper-gradient algorithms. This optimizer takes a Module and a gradient transform. At each step, the gradient of the module is passed through the transforms, and the module differentiably update -- i.e. when the next backward is called, gradients of both the module and the transform are computed. In turn, the transform can be updated via your favorite optmizer. Arguments model (Module) - Module to be updated. transform (Module) - Transform used to compute updates of the model. lr (float) - Learning rate. References Sutton. 1992. \u201cGain Adaptation Beats Least Squares.\u201d Schraudolph. 1999. \u201cLocal Gain Adaptation in Stochastic Gradient Descent.\u201d Baydin et al. 2017. \u201cOnline Learning Rate Adaptation with Hypergradient Descent.\u201d Majumder et al. 2019. \u201cLearning the Learning Rate for Gradient Descent by Gradient Descent.\u201d Jacobsen et al. 2019. \u201cMeta-Descent for Online, Continual Prediction.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear zero_grad \u00b6 1 LearnableOptimizer . zero_grad () Only reset target parameters. ParameterUpdate \u00b6 1 ParameterUpdate ( parameters , transform ) [Source] Description Convenience class to implement custom update functions. Objects instantiated from this class behave similarly to torch.autograd.grad , but return parameter updates as opposed to gradients. Concretely, the gradients are first computed, then fed to their respective transform whose output is finally returned to the user. Additionally, this class supports parameters that might not require updates by setting the allow_nograd flag to True. In this case, the returned update is None . Arguments parameters (list) - Parameters of the model to update. transform (callable) - A callable that returns an instantiated transform given a parameter. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 model = torch . nn . Linear () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) get_update = ParameterUpdate ( model , transform ) opt = torch . optim . SGD ( model . parameters () + get_update . parameters ()) for iteration in range ( 10 ): opt . zero_grad () error = loss ( model ( X ), y ) updates = get_update ( error , model . parameters (), create_graph = True , ) l2l . update_module ( model , updates ) opt . step () forward \u00b6 1 2 3 4 5 6 ParameterUpdate . forward ( loss , parameters , create_graph = False , retain_graph = False , allow_unused = False , allow_nograd = False ) Description Similar to torch.autograd.grad, but passes the gradients through the provided transform. Arguments loss (Tensor) - The loss to differentiate. parameters (iterable) - Parameters w.r.t. which we want to compute the update. create_graph (bool, optional , default=False) - Same as torch.autograd.grad . retain_graph (bool, optional , default=False) - Same as torch.autograd.grad . allow_unused (bool, optional , default=False) - Same as torch.autograd.grad . allow_nograd (bool, optional , default=False) - Properly handles parameters that do not require gradients. (Their update will be None .) DifferentiableSGD \u00b6 1 DifferentiableSGD ( lr ) [Source] Description A callable object that applies a list of updates to the parameters of a torch.nn.Module in a differentiable manner. For each parameter p and corresponding gradient g , calling an instance of this class results in updating parameters: p \\gets p - \\alpha g, where \\alpha is the learning rate. Note: The module is updated in-place. Arguments lr (float) - The learning rate used to update the model. Example 1 2 3 4 5 6 sgd = DifferentiableSGD ( 0.1 ) gradients = torch . autograd . grad ( loss , model . parameters (), create_gaph = True ) sgd ( model , gradients ) # model is updated in-place forward \u00b6 1 DifferentiableSGD . forward ( module , gradients = None ) Arguments module (Module) - The module to update. gradients (list, optional , default=None) - A list of gradients for each parameter of the module. If None, will use the gradients in .grad attributes. learn2learn.optim.transforms \u00b6 Optimization transforms are special modules that take gradients as inputs and output model updates. Transforms are usually parameterized, and those parameters can be learned by gradient descent, allow you to learn optimization functions from data. ModuleTransform \u00b6 1 ModuleTransform ( module_cls ) [Source] Description The ModuleTransform creates a an optimization transform based on any nn.Module. ModuleTransform automatically instanciates a module from its class, based on a given parameter. The input and output shapes are of the module are set to (1, param.numel()) . When optimizing large layers, this type of transform can quickly run out of memory. See KroneckerTransform for a scalable alternative. Arguments module_cls (callable) - A callable that instantiates the module used to transform gradients. Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) linear_transform = ModuleTransform ( torch . nn . Linear ) linear_update = linear_transform ( classifier . weight ) # maps gradients to updates, both of shape (1, 7840) loss ( classifier ( X ), y ) . backward () update = linear_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD. KroneckerTransform \u00b6 1 KroneckerTransform ( kronecker_cls , bias = False , psd = True ) [Source] Description The KroneckerTransform creates a an optimization transform based on nn.Module's that admit a Kronecker factorization. (see l2l.nn.Kronecker* ) Akin to the ModuleTransform, this class of transform instanciates a module from its class, based on a given parameter. But, instead of reshaping the gradients to shape (1, param.numel()) , this class assumes a Kronecker factorization of the weights for memory and computational efficiency. The specific dimension of the Kronecker factorization depends on the the parameter's shape. For a weight of shape (n, m), a KroneckerLinear transform consists of two weights with shapes (n, n) and (m, m) rather than a single weight of shape (nm, nm). Refer to Arnold et al., 2019 for more details. Arguments kronecker_cls (callable) - A callable that instantiates the Kronecker module used to transform gradients. References Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) kronecker_transform = KroneckerTransform ( l2l . nn . KroneckerLinear ) kronecker_update = kronecker_transform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = kronecker_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD. MetaCurvatureTransform \u00b6 1 MetaCurvatureTransform ( param , lr = 1.0 ) [Source] Description Implements the Meta-Curvature transform of Park and Oliva, 2019. Unlike ModuleTranform and KroneckerTransform , this class does not wrap other Modules but is directly called on a weight to instantiate the transform. Arguments param (Tensor) - The weight whose gradients will be transformed. lr (float, optional , default=1.0) - Scaling factor of the udpate. (non-learnable) References Park & Oliva. 2019. Meta-curvature. Example 1 2 3 4 5 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) metacurvature_update = MetaCurvatureTransform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = metacurvature_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"learn2learn.optim"},{"location":"docs/learn2learn.optim/#learn2learnoptim","text":"A set of utilities to write differentiable optimization algorithms.","title":"learn2learn.optim"},{"location":"docs/learn2learn.optim/#learnableoptimizer","text":"1 LearnableOptimizer ( model , transform , lr = 1.0 ) [Source] Description A PyTorch Optimizer with learnable transform, enabling the implementation of meta-descent / hyper-gradient algorithms. This optimizer takes a Module and a gradient transform. At each step, the gradient of the module is passed through the transforms, and the module differentiably update -- i.e. when the next backward is called, gradients of both the module and the transform are computed. In turn, the transform can be updated via your favorite optmizer. Arguments model (Module) - Module to be updated. transform (Module) - Transform used to compute updates of the model. lr (float) - Learning rate. References Sutton. 1992. \u201cGain Adaptation Beats Least Squares.\u201d Schraudolph. 1999. \u201cLocal Gain Adaptation in Stochastic Gradient Descent.\u201d Baydin et al. 2017. \u201cOnline Learning Rate Adaptation with Hypergradient Descent.\u201d Majumder et al. 2019. \u201cLearning the Learning Rate for Gradient Descent by Gradient Descent.\u201d Jacobsen et al. 2019. \u201cMeta-Descent for Online, Continual Prediction.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear","title":"LearnableOptimizer"},{"location":"docs/learn2learn.optim/#zero_grad","text":"1 LearnableOptimizer . zero_grad () Only reset target parameters.","title":"zero_grad"},{"location":"docs/learn2learn.optim/#parameterupdate","text":"1 ParameterUpdate ( parameters , transform ) [Source] Description Convenience class to implement custom update functions. Objects instantiated from this class behave similarly to torch.autograd.grad , but return parameter updates as opposed to gradients. Concretely, the gradients are first computed, then fed to their respective transform whose output is finally returned to the user. Additionally, this class supports parameters that might not require updates by setting the allow_nograd flag to True. In this case, the returned update is None . Arguments parameters (list) - Parameters of the model to update. transform (callable) - A callable that returns an instantiated transform given a parameter. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 model = torch . nn . Linear () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) get_update = ParameterUpdate ( model , transform ) opt = torch . optim . SGD ( model . parameters () + get_update . parameters ()) for iteration in range ( 10 ): opt . zero_grad () error = loss ( model ( X ), y ) updates = get_update ( error , model . parameters (), create_graph = True , ) l2l . update_module ( model , updates ) opt . step ()","title":"ParameterUpdate"},{"location":"docs/learn2learn.optim/#forward","text":"1 2 3 4 5 6 ParameterUpdate . forward ( loss , parameters , create_graph = False , retain_graph = False , allow_unused = False , allow_nograd = False ) Description Similar to torch.autograd.grad, but passes the gradients through the provided transform. Arguments loss (Tensor) - The loss to differentiate. parameters (iterable) - Parameters w.r.t. which we want to compute the update. create_graph (bool, optional , default=False) - Same as torch.autograd.grad . retain_graph (bool, optional , default=False) - Same as torch.autograd.grad . allow_unused (bool, optional , default=False) - Same as torch.autograd.grad . allow_nograd (bool, optional , default=False) - Properly handles parameters that do not require gradients. (Their update will be None .)","title":"forward"},{"location":"docs/learn2learn.optim/#differentiablesgd","text":"1 DifferentiableSGD ( lr ) [Source] Description A callable object that applies a list of updates to the parameters of a torch.nn.Module in a differentiable manner. For each parameter p and corresponding gradient g , calling an instance of this class results in updating parameters: p \\gets p - \\alpha g, where \\alpha is the learning rate. Note: The module is updated in-place. Arguments lr (float) - The learning rate used to update the model. Example 1 2 3 4 5 6 sgd = DifferentiableSGD ( 0.1 ) gradients = torch . autograd . grad ( loss , model . parameters (), create_gaph = True ) sgd ( model , gradients ) # model is updated in-place","title":"DifferentiableSGD"},{"location":"docs/learn2learn.optim/#forward_1","text":"1 DifferentiableSGD . forward ( module , gradients = None ) Arguments module (Module) - The module to update. gradients (list, optional , default=None) - A list of gradients for each parameter of the module. If None, will use the gradients in .grad attributes.","title":"forward"},{"location":"docs/learn2learn.optim/#learn2learnoptimtransforms","text":"Optimization transforms are special modules that take gradients as inputs and output model updates. Transforms are usually parameterized, and those parameters can be learned by gradient descent, allow you to learn optimization functions from data.","title":"learn2learn.optim.transforms"},{"location":"docs/learn2learn.optim/#moduletransform","text":"1 ModuleTransform ( module_cls ) [Source] Description The ModuleTransform creates a an optimization transform based on any nn.Module. ModuleTransform automatically instanciates a module from its class, based on a given parameter. The input and output shapes are of the module are set to (1, param.numel()) . When optimizing large layers, this type of transform can quickly run out of memory. See KroneckerTransform for a scalable alternative. Arguments module_cls (callable) - A callable that instantiates the module used to transform gradients. Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) linear_transform = ModuleTransform ( torch . nn . Linear ) linear_update = linear_transform ( classifier . weight ) # maps gradients to updates, both of shape (1, 7840) loss ( classifier ( X ), y ) . backward () update = linear_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"ModuleTransform"},{"location":"docs/learn2learn.optim/#kroneckertransform","text":"1 KroneckerTransform ( kronecker_cls , bias = False , psd = True ) [Source] Description The KroneckerTransform creates a an optimization transform based on nn.Module's that admit a Kronecker factorization. (see l2l.nn.Kronecker* ) Akin to the ModuleTransform, this class of transform instanciates a module from its class, based on a given parameter. But, instead of reshaping the gradients to shape (1, param.numel()) , this class assumes a Kronecker factorization of the weights for memory and computational efficiency. The specific dimension of the Kronecker factorization depends on the the parameter's shape. For a weight of shape (n, m), a KroneckerLinear transform consists of two weights with shapes (n, n) and (m, m) rather than a single weight of shape (nm, nm). Refer to Arnold et al., 2019 for more details. Arguments kronecker_cls (callable) - A callable that instantiates the Kronecker module used to transform gradients. References Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) kronecker_transform = KroneckerTransform ( l2l . nn . KroneckerLinear ) kronecker_update = kronecker_transform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = kronecker_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"KroneckerTransform"},{"location":"docs/learn2learn.optim/#metacurvaturetransform","text":"1 MetaCurvatureTransform ( param , lr = 1.0 ) [Source] Description Implements the Meta-Curvature transform of Park and Oliva, 2019. Unlike ModuleTranform and KroneckerTransform , this class does not wrap other Modules but is directly called on a weight to instantiate the transform. Arguments param (Tensor) - The weight whose gradients will be transformed. lr (float, optional , default=1.0) - Scaling factor of the udpate. (non-learnable) References Park & Oliva. 2019. Meta-curvature. Example 1 2 3 4 5 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) metacurvature_update = MetaCurvatureTransform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = metacurvature_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"MetaCurvatureTransform"},{"location":"docs/learn2learn.text/","text":"NewsClassification \u00b6 1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.text/#newsclassification","text":"1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.vision/","text":"learn2learn.vision \u00b6 Datasets, models, and other utilities related to computer vision. learn2learn.vision.models \u00b6 Description A set of commonly used models for meta-learning vision tasks. OmniglotFC \u00b6 1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ]) OmniglotCNN \u00b6 1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) MiniImagenetCNN \u00b6 1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) learn2learn.vision.datasets \u00b6 Description Some datasets commonly used in meta-learning vision tasks. FullOmniglot \u00b6 1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot ) MiniImagenet \u00b6 1 2 3 4 5 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) TieredImagenet \u00b6 1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FC100 \u00b6 1 2 3 4 5 FC100 ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) CIFARFS \u00b6 1 2 3 4 5 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) VGGFlower102 \u00b6 1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FGVCAircraft \u00b6 1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) learn2learn.vision.transforms \u00b6 Description A set of transformations commonly used in meta-learning vision tasks. RandomClassRotation \u00b6 1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ]) learn2learn.vision.benchmarks \u00b6 The benchmark modules provides a convenient interface to standardized benchmarks in the literature. It provides train/validation/test TaskDatasets and TaskTransforms for pre-defined datasets. This utility is useful for researchers to compare new algorithms against existing benchmarks. For a more fine-grained control over tasks and data, we recommend directly using l2l.data.TaskDataset and l2l.data.TaskTransforms . list_tasksets \u00b6 1 list_tasksets () [Source] Description Returns a list of all available benchmarks. Example 1 2 3 for name in l2l . vision . benchmarks . list_tasksets (): print ( name ) tasksets = l2l . vision . benchmarks . get_tasksets ( name ) get_tasksets \u00b6 1 2 3 4 5 6 7 8 9 get_tasksets ( name , train_ways = 5 , train_samples = 10 , test_ways = 5 , test_samples = 10 , num_tasks =- 1 , root = '~/data' , device = None , ** kwargs ) [Source] Description Returns the tasksets for a particular benchmark, using literature standard data and task transformations. The returned object is a namedtuple with attributes train , validation , test which correspond to their respective TaskDatasets. See examples/vision/maml_miniimagenet.py for an example. Arguments name (str) - The name of the benchmark. Full list in list_tasksets() . train_ways (int, optional , default=5) - The number of classes per train tasks. train_samples (int, optional , default=10) - The number of samples per train tasks. test_ways (int, optional , default=5) - The number of classes per test tasks. Also used for validation tasks. test_samples (int, optional , default=10) - The number of samples per test tasks. Also used for validation tasks. num_tasks (int, optional , default=-1) - The number of tasks in each TaskDataset. root (str, optional , default='~/data') - Where the data is stored. Example 1 2 3 4 5 6 7 train_tasks , validation_tasks , test_tasks = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = train_tasks . sample () or : tasksets = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = tasksets . train . sample ()","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvision","text":"Datasets, models, and other utilities related to computer vision.","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvisionmodels","text":"Description A set of commonly used models for meta-learning vision tasks.","title":"learn2learn.vision.models"},{"location":"docs/learn2learn.vision/#omniglotfc","text":"1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ])","title":"OmniglotFC"},{"location":"docs/learn2learn.vision/#omniglotcnn","text":"1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"OmniglotCNN"},{"location":"docs/learn2learn.vision/#miniimagenetcnn","text":"1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"MiniImagenetCNN"},{"location":"docs/learn2learn.vision/#learn2learnvisiondatasets","text":"Description Some datasets commonly used in meta-learning vision tasks.","title":"learn2learn.vision.datasets"},{"location":"docs/learn2learn.vision/#fullomniglot","text":"1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot )","title":"FullOmniglot"},{"location":"docs/learn2learn.vision/#miniimagenet","text":"1 2 3 4 5 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"MiniImagenet"},{"location":"docs/learn2learn.vision/#tieredimagenet","text":"1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"TieredImagenet"},{"location":"docs/learn2learn.vision/#fc100","text":"1 2 3 4 5 FC100 ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FC100"},{"location":"docs/learn2learn.vision/#cifarfs","text":"1 2 3 4 5 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"CIFARFS"},{"location":"docs/learn2learn.vision/#vggflower102","text":"1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"VGGFlower102"},{"location":"docs/learn2learn.vision/#fgvcaircraft","text":"1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FGVCAircraft"},{"location":"docs/learn2learn.vision/#learn2learnvisiontransforms","text":"Description A set of transformations commonly used in meta-learning vision tasks.","title":"learn2learn.vision.transforms"},{"location":"docs/learn2learn.vision/#randomclassrotation","text":"1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ])","title":"RandomClassRotation"},{"location":"docs/learn2learn.vision/#learn2learnvisionbenchmarks","text":"The benchmark modules provides a convenient interface to standardized benchmarks in the literature. It provides train/validation/test TaskDatasets and TaskTransforms for pre-defined datasets. This utility is useful for researchers to compare new algorithms against existing benchmarks. For a more fine-grained control over tasks and data, we recommend directly using l2l.data.TaskDataset and l2l.data.TaskTransforms .","title":"learn2learn.vision.benchmarks"},{"location":"docs/learn2learn.vision/#list_tasksets","text":"1 list_tasksets () [Source] Description Returns a list of all available benchmarks. Example 1 2 3 for name in l2l . vision . benchmarks . list_tasksets (): print ( name ) tasksets = l2l . vision . benchmarks . get_tasksets ( name )","title":"list_tasksets"},{"location":"docs/learn2learn.vision/#get_tasksets","text":"1 2 3 4 5 6 7 8 9 get_tasksets ( name , train_ways = 5 , train_samples = 10 , test_ways = 5 , test_samples = 10 , num_tasks =- 1 , root = '~/data' , device = None , ** kwargs ) [Source] Description Returns the tasksets for a particular benchmark, using literature standard data and task transformations. The returned object is a namedtuple with attributes train , validation , test which correspond to their respective TaskDatasets. See examples/vision/maml_miniimagenet.py for an example. Arguments name (str) - The name of the benchmark. Full list in list_tasksets() . train_ways (int, optional , default=5) - The number of classes per train tasks. train_samples (int, optional , default=10) - The number of samples per train tasks. test_ways (int, optional , default=5) - The number of classes per test tasks. Also used for validation tasks. test_samples (int, optional , default=10) - The number of samples per test tasks. Also used for validation tasks. num_tasks (int, optional , default=-1) - The number of tasks in each TaskDataset. root (str, optional , default='~/data') - Where the data is stored. Example 1 2 3 4 5 6 7 train_tasks , validation_tasks , test_tasks = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = train_tasks . sample () or : tasksets = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = tasksets . train . sample ()","title":"get_tasksets"},{"location":"pages/paper_list/","text":"Paper List \u00b6 The following papers were announced on the learn2learn Twitter account . You can submit unanounced papers through the following Google Form. (It does not matter if they are old or new, only that they have not been anounced already.) Google Form to anounce papers Submitted Papers \u00b6 Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples by Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, Hugo Larochelle We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. https://arxiv.org/abs/1903.03096 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks by Chelsea Finn, Pieter Abbeel, Sergey Levine Introduces the MAML loss for fast-adaptation. https://arxiv.org/abs/1703.03400 )])] Submission Form \u00b6 Loading\u2026","title":"Paper List"},{"location":"pages/paper_list/#paper-list","text":"The following papers were announced on the learn2learn Twitter account . You can submit unanounced papers through the following Google Form. (It does not matter if they are old or new, only that they have not been anounced already.) Google Form to anounce papers","title":"Paper List"},{"location":"pages/paper_list/#submitted-papers","text":"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples by Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, Hugo Larochelle We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. https://arxiv.org/abs/1903.03096 Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks by Chelsea Finn, Pieter Abbeel, Sergey Levine Introduces the MAML loss for fast-adaptation. https://arxiv.org/abs/1703.03400 )])]","title":"Submitted Papers"},{"location":"pages/paper_list/#submission-form","text":"Loading\u2026","title":"Submission Form"},{"location":"tutorials/getting_started/","text":"Getting Started \u00b6 learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules. Installing \u00b6 A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur. Development \u00b6 To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Getting Started"},{"location":"tutorials/getting_started/#getting-started","text":"learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules.","title":"Getting Started"},{"location":"tutorials/getting_started/#installing","text":"A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur.","title":"Installing"},{"location":"tutorials/getting_started/#development","text":"To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Development"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/","text":"Feature Reuse with ANIL \u00b6 Written by Ewina Pun on 3/30/2020. In this article, we will dive into a meta-learning algorithm called ANIL (Almost No Inner Loop) presented by Raghu et al., 2019 , and explain how to implement it with learn2learn. Note This tutorial is written for experienced PyTorch users who are getting started with meta-learning. Overview \u00b6 We look into how ANIL takes advantage of feature reuse for few-shot learning. ANIL simplifies MAML by removing the inner loop for all but the task-specific head of the underlying neural network. ANIL performs as well as MAML on benchmark few-shot classification and reinforcement learning tasks, and is computationally more efficient than MAML. We implement ANIL with learn2learn and provide additional results of how ANIL performs on other datasets. Lastly, we explain the implementation code step-be-step, making it easy for users to try ANIL on other datasets. ANIL algorithm \u00b6 Among various meta-learning algorithms for few-shot learning, MAML (model-agnostic meta-learning) (Finn et al. 2017) has been highly popular due to its substantial performance on several benchmarks. Its idea is to establish a meta-learner that seeks an initialization useful for fast learning of different tasks, then adapt to specific tasks quickly (within a few steps) and efficiently (with only a few examples). There are two types of parameter updates: the outer loop and the inner loop. The outer loop updates the meta-initialization of the neural network parameters to a setting that enables fast adaptation to new tasks. The inner loop takes the outer loop initialization and performs task-specific adaptation over a few labeled samples. To read more about meta-learning and MAML, you can read the summary article written by Finn on learning to learn and Lilian Weng's review on meta-learning . In 2019, Raghu et al. conjectured that we can obtain the same rapid learning performance of MAML solely through feature reuse. To test this hypothesis, they introduced ANIL (almost no inner loop), a simplified algorithm of MAML that is equally effective but computationally faster. Rapid learning vs. feature reuse Visualizations of rapid learning and feature reuse. Diagram from Raghu et al., 2019. Before we describe ANIL, we have to understand the difference between rapid learning and feature reuse. In rapid learning , the meta-initialization in the outer loop results in a parameter setting that is favorable for fast learning, thus significant adaptation to new tasks can rapidly take place in the inner loop. In feature reuse , the meta-initialization already contains useful features that can be reused, so little adaptation on the parameters is required in the inner loop. To prove feature reuse is a competitive alternative to rapid learning in MAML, the authors proposed a simplified algorithm, ANIL, where the inner loop is removed for all but the task-specific head of the underlying neural network during training and testing. ANIL vs. MAML Now, let us illustrate the difference mathematically. Let \\theta be the set of meta-initialization parameters for the feature extractable layers of the network and w be the meta-initialization parameters for the head. We obtain the label prediction \\hat{y} = w^T\\phi_\\theta(x) , where x is the input data and \\phi is a feature extractor parametrized by \\theta . Given \\theta_i and w_i at iteration step i , the outer loop updates both parameters via gradient descent: \\theta_{i+1} = \\theta_i - \\alpha\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) w_{i+1} = w_i - \\alpha\\nabla_{w_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) where \\mathcal{L}_\\tau is the loss computed for task \\tau , and \\alpha is the meta learning rate in the outer loop. Notice how the gradient is taken with respect to the initialization parameters w_i and \\theta_i , but the loss is computed on the adapted parameters \\theta_i^\\prime and w_i^\\prime . For one adaptation step in the inner loop, ANIL computes those adapted parameters as: \\theta_{i}^\\prime = \\theta_i w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) where \\beta is the learning rate of the inner loop. Concretely, ANIL keeps the feature extractor constant and only adapts the head with gradient descent. In contrast, MAML updates both the head and the feature extractor: \\theta_{i}^\\prime = \\theta_i - \\beta\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y). Unsurprisingly, ANIL is much more computationally efficient since it requires fewer updates in the inner loop. What might be surprising, is that this efficiency comes at almost no cost in terms of performance. Results ANIL provides fast adaptation in the absence of almost all inner loop parameter updates, while still matching the performance of MAML on few-shot image classification with Mini-ImageNet and Omniglot and standard reinforcement learning tasks. Method Omniglot-20way-1shot Omniglot-20way-5shot Mini-ImageNet-5way-1shot Mini-ImageNet-5way-5shot MAML 93.7 \u00b1 0.7 96.4 \u00b1 0.1 46.9 \u00b1 0.2 63.1 \u00b1 0.4 ANIL 96.2 \u00b1 0.5 98.0 \u00b1 0.3 46.7 \u00b1 0.4 61.5 \u00b1 0.5 Using ANIL with learn2learn \u00b6 With our understanding of how ANIL works, we are ready to implement the algorithm. An example implementation on the FC100 dataset is available at: anil_fc100.py . Using this implementation, we are able to obtain the following results on datasets such as Mini-ImageNet, CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn Mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% ANIL Implementation \u00b6 This section breaks down step-by-step the ANIL implementation with our example code. Creating dataset 1 2 3 4 train_dataset = l2l . vision . datasets . FC100 ( root = '~/data' , transform = tv . transforms . ToTensor (), mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) First, data are obtained and separated into train, validation and test dataset with l2l.vision.datasets.FC100 . tv.transforms.ToTensor() converts Python Imaging Library (PIL) images to PyTorch tensors. l2l.data.MetaDataset is a thin wrapper around torch datasets that automatically generates bookkeeping information to create new tasks. 1 2 3 4 5 6 7 8 9 train_transforms = [ FusedNWaysKShots ( train_dataset , n = ways , k = 2 * shots ), LoadData ( train_dataset ), RemapLabels ( train_dataset ), ConsecutiveLabels ( train_dataset ), ] train_tasks = l2l . data . TaskDataset ( train_dataset , task_transforms = train_transforms , num_tasks = 20000 ) l2l.data.TaskDataset creates a set of tasks from the MetaDataset using a list of task transformations: FusedNWaysKShots(dataset, n=ways, k=2*shots) : efficient implementation to keep k data samples from n randomly sampled labels. LoadData(dataset) : loads a sample from the dataset given its index. RemapLabels(dataset) : given samples from n classes, maps the labels to 0, \\dots, n . ConsecutiveLabels(dataset) : re-orders the samples in the task description such that they are sorted in consecutive order. Question Why k = 2*shots ? The number of samples k is twice the number of shots because one half of the samples are for adaption and the other half are for evaluation in the inner loop. Info For more details, please refer to the documentation of learn2learn.data . Creating model 1 2 3 4 5 6 features = l2l . vision . models . ConvBase ( output_size = 64 , channels = 3 , max_pool = True ) features = torch . nn . Sequential ( features , Lambda ( lambda x : x . view ( - 1 , 256 ))) features . to ( device ) head = torch . nn . Linear ( 256 , ways ) head = l2l . algorithms . MAML ( head , lr = fast_lr ) head . to ( device ) We then instantiate two modules, one for features and one for the head. ConvBase instantiates a four-layer CNN, and the head is a fully connected layer. Because we are not updating the feature extractor parameters, we only need to wrap the head with the l2l.algorithms.MAML() wrapper, which takes in the fast adaptation learning rate fast_lr used for the inner loop later. Info For more details on the MAML wrapper, please refer to the documentation of l2l.algorithms . Optimization setup 1 2 3 all_parameters = list ( features . parameters ()) + list ( head . parameters ()) optimizer = torch . optim . Adam ( all_parameters , lr = meta_lr ) loss = nn . CrossEntropyLoss ( reduction = 'mean' ) Next, we set up the optimizer with mini-batch SGD using torch.optim.Adam , which takes in both feature and head parameters, and learning rate meta_lr used for the outer loop. Outer loop 1 2 3 4 5 6 for iteration in range ( iters ): ... for task in range ( meta_bsz ): learner = head . clone () batch = train_tasks . sample () ... For training, validation and testing, we first sample a task, then copy the head with head.clone() , which is a method exposed by the MAML wrapper for PyTorch modules, akin to tensor.clone() for PyTorch tensors. Calling clone() allows us to update the parameters of the clone while maintaining ability to back-propagate to the parameters in head . There's no need for feature.clone() as we are only adapting the head. Inner loop 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def fast_adapt ( batch , learner , features , loss , adaptation_steps , shots , ways , device = None ): data , labels = batch data , labels = data . to ( device ), labels . to ( device ) data = features ( data ) # Separate data into adaptation/evaluation sets adaptation_indices = np . zeros ( data . size ( 0 ), dtype = bool ) adaptation_indices [ np . arange ( shots * ways ) * 2 ] = True evaluation_indices = torch . from_numpy ( ~ adaptation_indices ) adaptation_indices = torch . from_numpy ( adaptation_indices ) adaptation_data , adaptation_labels = data [ adaptation_indices ], labels [ adaptation_indices ] evaluation_data , evaluation_labels = data [ evaluation_indices ], labels [ evaluation_indices ] for step in range ( adaptation_steps ): train_error = loss ( learner ( adaptation_data ), adaptation_labels ) learner . adapt ( train_error ) predictions = learner ( evaluation_data ) valid_error = loss ( predictions , evaluation_labels ) valid_accuracy = accuracy ( predictions , evaluation_labels ) return valid_error , valid_accuracy In fast_adapt() , we separate data into adaptation and evaluation sets with k shot samples each. In each adaptation step, learner.adapt() takes a gradient step on the loss and updates the cloned parameter, learner , such that we can back-propagate through the adaptation step. Under the hood, this is achieved by calling torch.autograd.grad() and setting create_graph=True . fast_adapt() then returns the evaluation loss and accuracy based on the predicted and true labels. Question Why is the number of adaptation steps so small? To demonstrate fast adaptation, we want the algorithm to adapt to each specific task quickly within a few steps. Since the number of samples is so small in few-shot learning, increasing number of adaptation steps would not help raising the performance. Closing the outer loop 1 2 3 4 5 6 7 8 evaluation_error . backward () meta_train_error += evaluation_error . item () meta_train_accuracy += evaluation_accuracy . item () ... # Average the accumulated gradients and optimize for p in all_parameters : p . grad . data . mul_ ( 1.0 / meta_bsz ) optimizer . step () We compute the gradients with evaluation_error.backward() right after the inner loop updates to free activation and adaptation buffers from memory as early as possible. Lastly, after collecting the gradients, we average the accumulated gradients and updates the parameter at the end of each iteration with optimizer.step() . Conclusion \u00b6 Having explained the inner-workings of ANIL and its code implementation with learn2learn, I hope this tutorial will be helpful to those who are interested in using ANIL for their own research and applications. References \u00b6 Raghu, A., Raghu, M., Bengio, S., & Vinyals, O. (2019). Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1909.09157 Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1703.03400","title":"Feature Reuse with ANIL"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#feature-reuse-with-anil","text":"Written by Ewina Pun on 3/30/2020. In this article, we will dive into a meta-learning algorithm called ANIL (Almost No Inner Loop) presented by Raghu et al., 2019 , and explain how to implement it with learn2learn. Note This tutorial is written for experienced PyTorch users who are getting started with meta-learning.","title":"Feature Reuse with ANIL"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#overview","text":"We look into how ANIL takes advantage of feature reuse for few-shot learning. ANIL simplifies MAML by removing the inner loop for all but the task-specific head of the underlying neural network. ANIL performs as well as MAML on benchmark few-shot classification and reinforcement learning tasks, and is computationally more efficient than MAML. We implement ANIL with learn2learn and provide additional results of how ANIL performs on other datasets. Lastly, we explain the implementation code step-be-step, making it easy for users to try ANIL on other datasets.","title":"Overview"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#anil-algorithm","text":"Among various meta-learning algorithms for few-shot learning, MAML (model-agnostic meta-learning) (Finn et al. 2017) has been highly popular due to its substantial performance on several benchmarks. Its idea is to establish a meta-learner that seeks an initialization useful for fast learning of different tasks, then adapt to specific tasks quickly (within a few steps) and efficiently (with only a few examples). There are two types of parameter updates: the outer loop and the inner loop. The outer loop updates the meta-initialization of the neural network parameters to a setting that enables fast adaptation to new tasks. The inner loop takes the outer loop initialization and performs task-specific adaptation over a few labeled samples. To read more about meta-learning and MAML, you can read the summary article written by Finn on learning to learn and Lilian Weng's review on meta-learning . In 2019, Raghu et al. conjectured that we can obtain the same rapid learning performance of MAML solely through feature reuse. To test this hypothesis, they introduced ANIL (almost no inner loop), a simplified algorithm of MAML that is equally effective but computationally faster. Rapid learning vs. feature reuse Visualizations of rapid learning and feature reuse. Diagram from Raghu et al., 2019. Before we describe ANIL, we have to understand the difference between rapid learning and feature reuse. In rapid learning , the meta-initialization in the outer loop results in a parameter setting that is favorable for fast learning, thus significant adaptation to new tasks can rapidly take place in the inner loop. In feature reuse , the meta-initialization already contains useful features that can be reused, so little adaptation on the parameters is required in the inner loop. To prove feature reuse is a competitive alternative to rapid learning in MAML, the authors proposed a simplified algorithm, ANIL, where the inner loop is removed for all but the task-specific head of the underlying neural network during training and testing. ANIL vs. MAML Now, let us illustrate the difference mathematically. Let \\theta be the set of meta-initialization parameters for the feature extractable layers of the network and w be the meta-initialization parameters for the head. We obtain the label prediction \\hat{y} = w^T\\phi_\\theta(x) , where x is the input data and \\phi is a feature extractor parametrized by \\theta . Given \\theta_i and w_i at iteration step i , the outer loop updates both parameters via gradient descent: \\theta_{i+1} = \\theta_i - \\alpha\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) w_{i+1} = w_i - \\alpha\\nabla_{w_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) where \\mathcal{L}_\\tau is the loss computed for task \\tau , and \\alpha is the meta learning rate in the outer loop. Notice how the gradient is taken with respect to the initialization parameters w_i and \\theta_i , but the loss is computed on the adapted parameters \\theta_i^\\prime and w_i^\\prime . For one adaptation step in the inner loop, ANIL computes those adapted parameters as: \\theta_{i}^\\prime = \\theta_i w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) where \\beta is the learning rate of the inner loop. Concretely, ANIL keeps the feature extractor constant and only adapts the head with gradient descent. In contrast, MAML updates both the head and the feature extractor: \\theta_{i}^\\prime = \\theta_i - \\beta\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y). Unsurprisingly, ANIL is much more computationally efficient since it requires fewer updates in the inner loop. What might be surprising, is that this efficiency comes at almost no cost in terms of performance. Results ANIL provides fast adaptation in the absence of almost all inner loop parameter updates, while still matching the performance of MAML on few-shot image classification with Mini-ImageNet and Omniglot and standard reinforcement learning tasks. Method Omniglot-20way-1shot Omniglot-20way-5shot Mini-ImageNet-5way-1shot Mini-ImageNet-5way-5shot MAML 93.7 \u00b1 0.7 96.4 \u00b1 0.1 46.9 \u00b1 0.2 63.1 \u00b1 0.4 ANIL 96.2 \u00b1 0.5 98.0 \u00b1 0.3 46.7 \u00b1 0.4 61.5 \u00b1 0.5","title":"ANIL algorithm"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#using-anil-with-learn2learn","text":"With our understanding of how ANIL works, we are ready to implement the algorithm. An example implementation on the FC100 dataset is available at: anil_fc100.py . Using this implementation, we are able to obtain the following results on datasets such as Mini-ImageNet, CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn Mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6%","title":"Using ANIL with learn2learn"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#anil-implementation","text":"This section breaks down step-by-step the ANIL implementation with our example code. Creating dataset 1 2 3 4 train_dataset = l2l . vision . datasets . FC100 ( root = '~/data' , transform = tv . transforms . ToTensor (), mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) First, data are obtained and separated into train, validation and test dataset with l2l.vision.datasets.FC100 . tv.transforms.ToTensor() converts Python Imaging Library (PIL) images to PyTorch tensors. l2l.data.MetaDataset is a thin wrapper around torch datasets that automatically generates bookkeeping information to create new tasks. 1 2 3 4 5 6 7 8 9 train_transforms = [ FusedNWaysKShots ( train_dataset , n = ways , k = 2 * shots ), LoadData ( train_dataset ), RemapLabels ( train_dataset ), ConsecutiveLabels ( train_dataset ), ] train_tasks = l2l . data . TaskDataset ( train_dataset , task_transforms = train_transforms , num_tasks = 20000 ) l2l.data.TaskDataset creates a set of tasks from the MetaDataset using a list of task transformations: FusedNWaysKShots(dataset, n=ways, k=2*shots) : efficient implementation to keep k data samples from n randomly sampled labels. LoadData(dataset) : loads a sample from the dataset given its index. RemapLabels(dataset) : given samples from n classes, maps the labels to 0, \\dots, n . ConsecutiveLabels(dataset) : re-orders the samples in the task description such that they are sorted in consecutive order. Question Why k = 2*shots ? The number of samples k is twice the number of shots because one half of the samples are for adaption and the other half are for evaluation in the inner loop. Info For more details, please refer to the documentation of learn2learn.data . Creating model 1 2 3 4 5 6 features = l2l . vision . models . ConvBase ( output_size = 64 , channels = 3 , max_pool = True ) features = torch . nn . Sequential ( features , Lambda ( lambda x : x . view ( - 1 , 256 ))) features . to ( device ) head = torch . nn . Linear ( 256 , ways ) head = l2l . algorithms . MAML ( head , lr = fast_lr ) head . to ( device ) We then instantiate two modules, one for features and one for the head. ConvBase instantiates a four-layer CNN, and the head is a fully connected layer. Because we are not updating the feature extractor parameters, we only need to wrap the head with the l2l.algorithms.MAML() wrapper, which takes in the fast adaptation learning rate fast_lr used for the inner loop later. Info For more details on the MAML wrapper, please refer to the documentation of l2l.algorithms . Optimization setup 1 2 3 all_parameters = list ( features . parameters ()) + list ( head . parameters ()) optimizer = torch . optim . Adam ( all_parameters , lr = meta_lr ) loss = nn . CrossEntropyLoss ( reduction = 'mean' ) Next, we set up the optimizer with mini-batch SGD using torch.optim.Adam , which takes in both feature and head parameters, and learning rate meta_lr used for the outer loop. Outer loop 1 2 3 4 5 6 for iteration in range ( iters ): ... for task in range ( meta_bsz ): learner = head . clone () batch = train_tasks . sample () ... For training, validation and testing, we first sample a task, then copy the head with head.clone() , which is a method exposed by the MAML wrapper for PyTorch modules, akin to tensor.clone() for PyTorch tensors. Calling clone() allows us to update the parameters of the clone while maintaining ability to back-propagate to the parameters in head . There's no need for feature.clone() as we are only adapting the head. Inner loop 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def fast_adapt ( batch , learner , features , loss , adaptation_steps , shots , ways , device = None ): data , labels = batch data , labels = data . to ( device ), labels . to ( device ) data = features ( data ) # Separate data into adaptation/evaluation sets adaptation_indices = np . zeros ( data . size ( 0 ), dtype = bool ) adaptation_indices [ np . arange ( shots * ways ) * 2 ] = True evaluation_indices = torch . from_numpy ( ~ adaptation_indices ) adaptation_indices = torch . from_numpy ( adaptation_indices ) adaptation_data , adaptation_labels = data [ adaptation_indices ], labels [ adaptation_indices ] evaluation_data , evaluation_labels = data [ evaluation_indices ], labels [ evaluation_indices ] for step in range ( adaptation_steps ): train_error = loss ( learner ( adaptation_data ), adaptation_labels ) learner . adapt ( train_error ) predictions = learner ( evaluation_data ) valid_error = loss ( predictions , evaluation_labels ) valid_accuracy = accuracy ( predictions , evaluation_labels ) return valid_error , valid_accuracy In fast_adapt() , we separate data into adaptation and evaluation sets with k shot samples each. In each adaptation step, learner.adapt() takes a gradient step on the loss and updates the cloned parameter, learner , such that we can back-propagate through the adaptation step. Under the hood, this is achieved by calling torch.autograd.grad() and setting create_graph=True . fast_adapt() then returns the evaluation loss and accuracy based on the predicted and true labels. Question Why is the number of adaptation steps so small? To demonstrate fast adaptation, we want the algorithm to adapt to each specific task quickly within a few steps. Since the number of samples is so small in few-shot learning, increasing number of adaptation steps would not help raising the performance. Closing the outer loop 1 2 3 4 5 6 7 8 evaluation_error . backward () meta_train_error += evaluation_error . item () meta_train_accuracy += evaluation_accuracy . item () ... # Average the accumulated gradients and optimize for p in all_parameters : p . grad . data . mul_ ( 1.0 / meta_bsz ) optimizer . step () We compute the gradients with evaluation_error.backward() right after the inner loop updates to free activation and adaptation buffers from memory as early as possible. Lastly, after collecting the gradients, we average the accumulated gradients and updates the parameter at the end of each iteration with optimizer.step() .","title":"ANIL Implementation"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#conclusion","text":"Having explained the inner-workings of ANIL and its code implementation with learn2learn, I hope this tutorial will be helpful to those who are interested in using ANIL for their own research and applications.","title":"Conclusion"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#references","text":"Raghu, A., Raghu, M., Bengio, S., & Vinyals, O. (2019). Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1909.09157 Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1703.03400","title":"References"}]}